{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_tr = Path(\"../ClinicalTransformerMRC/example_datasets/2018n2c2/raw_data/train\")\n",
    "p_dev = Path(\"../ClinicalTransformerMRC/example_datasets/2018n2c2/raw_data/test/\")\n",
    "\n",
    "fids = []\n",
    "\n",
    "fids.extend([f for f in p_tr.glob(\"*.ann\")])\n",
    "fids.extend([f for f in p_dev.glob(\"*.ann\")])\n",
    "\n",
    "len(fids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle as pkl\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import permutations, combinations\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# https://github.com/uf-hobi-informatics-lab/NLPreprocessing (git clone this repo to local)\n",
    "sys.path.append(\"./NLPreprocessing/\")\n",
    "sys.path.append(\"./NLPreprocessing/text_process\")\n",
    "from annotation2BIO import pre_processing, read_annotation_brat, generate_BIO\n",
    "MIMICIII_PATTERN = \"\"\n",
    "from sentence_tokenization import logger as l1\n",
    "from annotation2BIO import logger as l2\n",
    "l1.disabled = True\n",
    "l2.disabled = True\n",
    "\n",
    "def pkl_save(data, file):\n",
    "    with open(file, \"wb\") as f:\n",
    "        pkl.dump(data, f)\n",
    "\n",
    "        \n",
    "def pkl_load(file):\n",
    "    with open(file, \"rb\") as f:\n",
    "        data = pkl.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_text(ifn):\n",
    "    with open(ifn, \"r\") as f:\n",
    "        txt = f.read()\n",
    "    return txt\n",
    "\n",
    "\n",
    "def save_text(text, ofn):\n",
    "    with open(ofn, \"w\") as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_entity_to_sent_mapping(nnsents, entities, idx2e, fn):\n",
    "    loc_ens = []\n",
    "    \n",
    "    ll = len(nnsents)\n",
    "    mapping = defaultdict(list)\n",
    "    for idx, each in enumerate(entities):\n",
    "        en_label = idx2e[idx]\n",
    "        en_s = each[2][0]\n",
    "        en_e = each[2][1]\n",
    "        new_en = []\n",
    "        \n",
    "        i = 0\n",
    "        while i < ll and nnsents[i][1][0] < en_s:\n",
    "            i += 1\n",
    "        s_s = nnsents[i][1][0]\n",
    "        s_e = nnsents[i][1][1]\n",
    "\n",
    "        if en_s == s_s:\n",
    "            mapping[en_label].append(i)\n",
    "\n",
    "            while i < ll and s_e < en_e:\n",
    "                i += 1\n",
    "                s_e = nnsents[i][1][1]\n",
    "            if s_e == en_e:\n",
    "                 mapping[en_label].append(i)\n",
    "            else:\n",
    "                mapping[en_label].append(i)\n",
    "                print(fn)\n",
    "                print(\"last index not match \", each)\n",
    "        else:\n",
    "            mapping[en_label].append(i)\n",
    "            print(fn)\n",
    "            print(\"first index not match \", each)\n",
    "\n",
    "            while i < ll and s_e < en_e:\n",
    "                i += 1\n",
    "                s_e = nnsents[i][1][1]\n",
    "            if s_e == en_e:\n",
    "                 mapping[en_label].append(i)\n",
    "            else:\n",
    "                mapping[en_label].append(i)\n",
    "                print(fn)\n",
    "                print(\"last index not match \", each)\n",
    "    return mapping\n",
    "\n",
    "    \n",
    "def __ann_info(ann):\n",
    "    en_info = ann.split(\" \")\n",
    "    return en_info[0], int(en_info[1]), int(en_info[-1])\n",
    "\n",
    "\n",
    "def load_annotation_brat(ann_file, rep=False):\n",
    "    \"\"\"\n",
    "    load annotation data\n",
    "    entity_id2index_map -> {'T1': 0}\n",
    "    entites -> ('T1', 'anticoagulant medications', 'Drug', (1000, 1025))\n",
    "    relations -> ('Route-Drug', 'Arg1:T3', 'Arg2:T2')\n",
    "    \"\"\"\n",
    "    # map the entity id (e.g., T1) to its index in entities list\n",
    "    entity_id2index_map = dict()\n",
    "    entites = []\n",
    "    relations = []\n",
    "    events = []\n",
    "    attrs = []\n",
    "    \n",
    "    with open(ann_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            anns = line.split(\"\\t\")\n",
    "            ann_id = anns[0]\n",
    "            if ann_id.startswith(\"T\"):\n",
    "                # T1\tLivingStatus 25 30\tlives\n",
    "                entity_words = anns[-1]\n",
    "                t_type, offset_s, offset_e = __ann_info(anns[1])\n",
    "                entites.append((entity_words, t_type, (offset_s, offset_e), ann_id))\n",
    "                entity_id2index_map[ann_id] = len(entites) - 1\n",
    "\n",
    "            elif ann_id.startswith(\"A\"):\n",
    "                #  A1\tStatusTimeVal T2 current\n",
    "                att_type, env_id, att_val = anns[1].strip().split(\" \")\n",
    "                attrs.append((att_type, env_id, att_val))\n",
    "\n",
    "            elif ann_id.startswith(\"E\"):\n",
    "                # E2\tAlcohol:T3 Status:T4 Amount:T5 Frequency:T6 Type:T10\n",
    "                single_event = dict()\n",
    "                envs = anns[1].split(\" \")\n",
    "                trigger = envs[0].strip().split(\":\")\n",
    "                tas = []\n",
    "                for each in envs[1:]:\n",
    "                    tas.append(each.strip().split(\":\"))\n",
    "                \n",
    "                single_event[\"trigger\"] = trigger\n",
    "                single_event[\"events\"] = tas\n",
    "                    \n",
    "                events.append(single_event)\n",
    "            elif ann_id.startswith(\"R\"):\n",
    "                # R2\tStrength-Drug Arg1:T6 Arg2:T5\n",
    "                relation, tail, head = anns[1].split()[0], anns[1].split()[1], anns[1].split()[-1]\n",
    "                \n",
    "                relations.append((relation, tail, head))\n",
    "\n",
    "    return entity_id2index_map, entites, relations, events, attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "sample mrc format data:\n",
    "a list of dict as\n",
    "  {\n",
    "    \"context\": \"Germany 's representative to the European Union 's veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer .\",\n",
    "    \"end_position\": [\n",
    "      0,\n",
    "      23\n",
    "    ],\n",
    "    \"entity_label\": \"LOC\",\n",
    "    \"impossible\": false,\n",
    "    \"qas_id\": \"4.3\",\n",
    "    \"query\": \"location entities are the name of politically or geographically defined locations such as cities, provinces, countries, international regions, bodies of water, mountains, etc.\",\n",
    "    \"span_position\": [\n",
    "      \"0;0\",\n",
    "      \"23;23\"\n",
    "    ],\n",
    "    \"start_position\": [\n",
    "      0,\n",
    "      23\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"context\": \"EU rejects German call to boycott British lamb .\",\n",
    "    \"end_position\": [],\n",
    "    \"entity_label\": \"PER\",\n",
    "    \"impossible\": true,\n",
    "    \"qas_id\": \"0.2\",\n",
    "    \"query\": \"person entities are named persons or family.\",\n",
    "    \"span_position\": [],\n",
    "    \"start_position\": []\n",
    "  }, ...\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# create query template\n",
    "# make sure the label id in qas_id is consistent for each type of entity\n",
    "entity_query_template = {\n",
    "    \"Drug\": \"Find the drug events including names, brand names, and collective names.\",\n",
    "    \"Strength\": \"Find the strength events that are the amount of drug in a given dosage.\",\n",
    "    \"Form\": \"Find the form events that are the physical form of given drug or medication.\", \n",
    "    \"Dosage\": \"Find the dosage events that are the amount of a medication used in each administration.\", \n",
    "    \"Frequency\":\"Frequency indicates how often each dose of the medication should be taken\", \n",
    "    \"Route\": \"Find the frequency events that indicate how often each dose of the medication should be taken.\", \n",
    "    \"Duration\": \"Find the duration events that indicate how long the medication is to be administered.\", \n",
    "    \"Reason\": \"Find the reason events that are the medical reason for which the medication is given.\",\n",
    "    \"ADE\": \"Find the ADE events that are injuries resulting from a medical intervention related to drugs.\"\n",
    "}\n",
    "\n",
    "attribute_query_template = {\n",
    "    \"Strength\": \"What is the active ingredient amount of {}\",\n",
    "    \"Form\": \"What is the physical form of {}\", \n",
    "    \"Dosage\": \"What is the amount of {} taken\", \n",
    "    \"Frequency\":\"How often each dose of {} should be taken\", \n",
    "    \"Route\": \"What is the path of {} taken into the body\", \n",
    "    \"Duration\": \"How long to take {}\", \n",
    "    \"Reason\": \"What is the medical reason for giving {}\",\n",
    "    \"ADE\": \"What are the injuries resulting from the use of {}\"\n",
    "}\n",
    "\n",
    "entity_id = {\n",
    "    \"Drug\": 1, \"Strength\": 2, \"Form\": 3, \"Dosage\": 4, \"Frequency\": 5, \"Route\": 6, \"Duration\": 7, \"Reason\": 8, \"ADE\": 9\n",
    "}\n",
    "\n",
    "attribute_id = {\n",
    "    \"Strength\": 1, \"Form\": 2, \"Dosage\": 3, \"Frequency\": 4, \"Route\": 5, \"Duration\": 6, \"Reason\": 7, \"ADE\": 8\n",
    "}\n",
    "\n",
    "head_entity = {\"Drug\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent(sents, idx1, idx2):\n",
    "    if idx1 == idx2:\n",
    "        return sents[idx1]\n",
    "    elif idx1 == idx2 + 1:\n",
    "        raise Exception(f\"{idx1} {idx2} - entity not in the same sentence\")\n",
    "    else:\n",
    "        raise Exception(f\"{idx1} {idx2} - the entity has word spread in >2 sentences\")\n",
    "        \n",
    "        \n",
    "def to_json(data, p, fn=\"train\"):\n",
    "    import json\n",
    "    \n",
    "    ofn = p / f\"mrc-ner.{fn}\"\n",
    "    \n",
    "    with open(ofn, \"w\") as f:\n",
    "        json.dump(data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create mrc format data using the brat annotation file\n",
    "#NER task\n",
    "\n",
    "training_data = []\n",
    "\n",
    "for i, fn in enumerate(p_tr.glob(\"*.ann\")):\n",
    "\n",
    "    txt_fn = p_tr / f\"{fn.stem}.txt\"\n",
    "    ann_fn = p_tr / f\"{fn.name}\"\n",
    "    txt, sents = pre_processing(txt_fn, MIMICIII_PATTERN, max_len=256)\n",
    "    e2i, ens, relations, evns, attrs = load_annotation_brat(ann_fn)\n",
    "    i2e = {v: k for k, v in e2i.items()}\n",
    "    nsents, sent_bound = generate_BIO(sents, ens, file_id=\"\", no_overlap=False, record_pos=True)\n",
    "    nnsents = [w for sent in nsents for w in sent]\n",
    "    mappings = create_entity_to_sent_mapping(nnsents, ens, i2e, fn.name)\n",
    "\n",
    "    num_sents = len(nsents)\n",
    "    sent_ids = set(range(num_sents))\n",
    "    sent_with_entities = set()\n",
    "\n",
    "    entity_sent_idx_mappings = defaultdict(list)\n",
    "\n",
    "    # sentence with entities\n",
    "    for en in ens:\n",
    "\n",
    "        entype = en[1]\n",
    "        type_id = entity_id[entype]\n",
    "        \n",
    "        s_idx, e_idx = mappings[en[-1]]\n",
    "        word_info1 = nnsents[s_idx]\n",
    "        word_info2 = nnsents[e_idx]\n",
    "        \n",
    "        sent_idx1 = word_info1[3][0]\n",
    "        sent_idx2 = word_info2[3][0]\n",
    "        sent_with_entities.add(sent_idx1)\n",
    "    #         sent_with_entities.add(sent_idx2)\n",
    "        \n",
    "        try:\n",
    "            sent_text = get_sent(nsents, sent_idx1, sent_idx2)\n",
    "            context = \" \".join([e[0] for e in sent_text])\n",
    "            start = word_info1[3][1]\n",
    "            end = word_info2[3][1]\n",
    "            start_end = f\"{start};{end}\"\n",
    "        except Exception as ex:\n",
    "            print(fn.name)\n",
    "            print(word_info1, word_info2)\n",
    "            print(context)\n",
    "            print(start, end, start_end)\n",
    "            print(entype, type_id)\n",
    "        \n",
    "        # key will be sent_id, type\n",
    "        # data will be tuple (context, entype, type_id, start, end, start_end)\n",
    "        entity_sent_idx_mappings[(sent_idx1, entype)].append(\n",
    "            (context, entype, type_id, start, end, start_end))\n",
    "\n",
    "    # for i in sent_ids:\n",
    "    for i in sent_with_entities:\n",
    "        file_id = f\"{fn.stem}\"\n",
    "        sent_id = f\"{i}\"\n",
    "        sent_i_context = \" \".join(e[0] for e in nsents[i])\n",
    "        for k, v in entity_query_template.items():\n",
    "            tid = entity_id[k]\n",
    "            \n",
    "            if (i, k) in entity_sent_idx_mappings:\n",
    "                d = {\n",
    "                        \"context\": sent_i_context,\n",
    "                        \"end_position\": [],\n",
    "                        \"entity_label\": k,\n",
    "                        \"impossible\": False,\n",
    "                        \"qas_id\": f\"{file_id}.{sent_id}.{sent_id}.{tid}\",\n",
    "                        \"query\": v,\n",
    "                        \"span_position\": [],\n",
    "                        \"start_position\": []\n",
    "                        }\n",
    "                \n",
    "                entities = entity_sent_idx_mappings[(i, k)]\n",
    "        \n",
    "                for ent in entities:\n",
    "                    assert ent[0] == sent_i_context, f\"expect context: {ent[0]} but get {sent_i_context}\"\n",
    "                    assert ent[1] == k, f\"expect en type: {ent[1]} but get {k}\"\n",
    "                    s, e, se = ent[-3:]\n",
    "                    d[\"start_position\"].append(s)\n",
    "                    d[\"end_position\"].append(e)\n",
    "                    d[\"span_position\"].append(se)\n",
    "                \n",
    "                training_data.append(d)\n",
    "            else:\n",
    "                d = {\n",
    "                        \"context\": sent_i_context,\n",
    "                        \"end_position\": [],\n",
    "                        \"entity_label\": k,\n",
    "                        \"impossible\": True,\n",
    "                        \"qas_id\": f\"{file_id}.{sent_id}.{sent_id}.{tid}\",\n",
    "                        \"query\": v,\n",
    "                        \"span_position\": [],\n",
    "                        \"start_position\": []\n",
    "                    }\n",
    "                training_data.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pout = Path(\"../ClinicalTransformerMRC/example_datasets/2018n2c2/mrc_data/entity/\")\n",
    "pout.mkdir(parents=True, exist_ok=True)\n",
    "to_json(training_data, pout, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create dev data\n",
    "#NER task\n",
    "dev_data = []\n",
    "\n",
    "\n",
    "for i, fn in enumerate(p_dev.glob(\"*.ann\")):\n",
    "\n",
    "    txt_fn = p_dev / f\"{fn.stem}.txt\"\n",
    "    ann_fn = p_dev / f\"{fn.name}\"\n",
    "    txt, sents = pre_processing(txt_fn, MIMICIII_PATTERN, max_len=256)\n",
    "    e2i, ens, _, evns, attrs = load_annotation_brat(ann_fn)\n",
    "    i2e = {v: k for k, v in e2i.items()}\n",
    "    nsents, sent_bound = generate_BIO(sents, ens, file_id=\"\", no_overlap=False, record_pos=True)\n",
    "    nnsents = [w for sent in nsents for w in sent]\n",
    "    mappings = create_entity_to_sent_mapping(nnsents, ens, i2e, fn.name)\n",
    "\n",
    "    num_sents = len(nsents)\n",
    "    sent_ids = set(range(num_sents))\n",
    "    sent_with_entities = set()\n",
    "\n",
    "    entity_sent_idx_mappings = defaultdict(list)\n",
    "\n",
    "    # sentence with entities\n",
    "    for en in ens:\n",
    "\n",
    "        entype = en[1]\n",
    "        type_id = entity_id[entype]\n",
    "        \n",
    "        s_idx, e_idx = mappings[en[-1]]\n",
    "        word_info1 = nnsents[s_idx]\n",
    "        word_info2 = nnsents[e_idx]\n",
    "        \n",
    "        sent_idx1 = word_info1[3][0]\n",
    "        sent_idx2 = word_info2[3][0]\n",
    "        sent_with_entities.add(sent_idx1)\n",
    "    #         sent_with_entities.add(sent_idx2)\n",
    "        \n",
    "        try:\n",
    "            sent_text = get_sent(nsents, sent_idx1, sent_idx2)\n",
    "            context = \" \".join([e[0] for e in sent_text])\n",
    "            start = word_info1[3][1]\n",
    "            end = word_info2[3][1]\n",
    "            start_end = f\"{start};{end}\"\n",
    "        except Exception as ex:\n",
    "            print(fn.name)\n",
    "            print(word_info1, word_info2)\n",
    "            print(context)\n",
    "            print(start, end, start_end)\n",
    "            print(entype, type_id)\n",
    "        \n",
    "        # key will be sent_id, type\n",
    "        # data will be tuple (context, entype, type_id, start, end, start_end)\n",
    "        entity_sent_idx_mappings[(sent_idx1, entype)].append(\n",
    "            (context, entype, type_id, start, end, start_end))\n",
    "\n",
    "    # for i in sent_ids:\n",
    "    for i in sent_with_entities:\n",
    "        file_id = f\"{fn.stem}\"\n",
    "        sent_id = f\"{i}\"\n",
    "        sent_i_context = \" \".join(e[0] for e in nsents[i])\n",
    "        for k, v in entity_query_template.items():\n",
    "            tid = entity_id[k]\n",
    "            if (i, k) in entity_sent_idx_mappings:\n",
    "                d = {\n",
    "                        \"context\": sent_i_context,\n",
    "                        \"end_position\": [],\n",
    "                        \"entity_label\": k,\n",
    "                        \"impossible\": False,\n",
    "                        \"qas_id\": f\"{file_id}.{sent_id}.{sent_id}.{tid}\",\n",
    "                        \"query\": v,\n",
    "                        \"span_position\": [],\n",
    "                        \"start_position\": []\n",
    "                        }\n",
    "                \n",
    "                entities = entity_sent_idx_mappings[(i, k)]\n",
    "        \n",
    "                for ent in entities:\n",
    "                    assert ent[0] == sent_i_context, f\"expect context: {ent[0]} but get {sent_i_context}\"\n",
    "                    assert ent[1] == k, f\"expect en type: {ent[1]} but get {k}\"\n",
    "                    s, e, se = ent[-3:]\n",
    "                    d[\"start_position\"].append(s)\n",
    "                    d[\"end_position\"].append(e)\n",
    "                    d[\"span_position\"].append(se)\n",
    "                \n",
    "                dev_data.append(d)\n",
    "            else:\n",
    "                d = {\n",
    "                        \"context\": sent_i_context,\n",
    "                        \"end_position\": [],\n",
    "                        \"entity_label\": k,\n",
    "                        \"impossible\": True,\n",
    "                        \"qas_id\": f\"{file_id}.{sent_id}.{sent_id}.{tid}\",\n",
    "                        \"query\": v,\n",
    "                        \"span_position\": [],\n",
    "                        \"start_position\": []\n",
    "                    }\n",
    "                dev_data.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pout = Path(\"/data/datasets/cheng/ClinicalTransformerMRC/2018n2c2/dataset/mrc_entity/\")\n",
    "pout.mkdir(parents=True, exist_ok=True)\n",
    "to_json(dev_data, pout, \"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create test data\n",
    "#NER task\n",
    "test_data = []\n",
    "\n",
    "for i, fn in enumerate(p_dev.glob(\"*.ann\")):\n",
    "\n",
    "    txt_fn = p_dev / f\"{fn.stem}.txt\"\n",
    "    ann_fn = p_dev / f\"{fn.name}\"\n",
    "    txt, sents = pre_processing(txt_fn, MIMICIII_PATTERN, max_len=256)\n",
    "    e2i, ens, _, evns, attrs = load_annotation_brat(ann_fn)\n",
    "    i2e = {v: k for k, v in e2i.items()}\n",
    "    nsents, sent_bound = generate_BIO(sents, ens, file_id=\"\", no_overlap=False, record_pos=True)\n",
    "    nnsents = [w for sent in nsents for w in sent]\n",
    "    mappings = create_entity_to_sent_mapping(nnsents, ens, i2e, fn.name)\n",
    "\n",
    "    num_sents = len(nsents)\n",
    "    sent_ids = set(range(num_sents))\n",
    "    sent_with_entities = set()\n",
    "\n",
    "    entity_sent_idx_mappings = defaultdict(list)\n",
    "\n",
    "    # sentence with entities\n",
    "    for en in ens:\n",
    "\n",
    "        entype = en[1]\n",
    "        type_id = entity_id[entype]\n",
    "        \n",
    "        s_idx, e_idx = mappings[en[-1]]\n",
    "        word_info1 = nnsents[s_idx]\n",
    "        word_info2 = nnsents[e_idx]\n",
    "        \n",
    "        sent_idx1 = word_info1[3][0]\n",
    "        sent_idx2 = word_info2[3][0]\n",
    "        sent_with_entities.add(sent_idx1)\n",
    "    #         sent_with_entities.add(sent_idx2)\n",
    "\n",
    "    # for i in sent_ids:\n",
    "    for i in sent_with_entities:\n",
    "        file_id = f\"{fn.stem}\"\n",
    "        sent_id = f\"{i}\"\n",
    "        sent_i_context = \" \".join(e[0] for e in nsents[i])\n",
    "        for k, v in entity_query_template.items():\n",
    "            tid = entity_id[k]\n",
    "            d = {\n",
    "                    \"context\": sent_i_context,\n",
    "                    \"end_position\": [],\n",
    "                    \"entity_label\": k,\n",
    "                    \"impossible\": True,\n",
    "                    \"qas_id\": f\"{file_id}.{sent_id}.{sent_id}.{tid}\",\n",
    "                    \"query\": v,\n",
    "                    \"span_position\": [],\n",
    "                    \"start_position\": []\n",
    "                    }\n",
    "            test_data.append(d)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pout = Path(\"/data/datasets/cheng/ClinicalTransformerMRC/2018n2c2/dataset/mrc_entity/\")\n",
    "pout.mkdir(parents=True, exist_ok=True)\n",
    "to_json(test_data, pout, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create mrc format data using the brat annotation file\n",
    "#RE task\n",
    "training_data = []\n",
    "\n",
    "for i, fn in enumerate(p_tr.glob(\"*.ann\")):\n",
    "\n",
    "    txt_fn = p_tr / f\"{fn.stem}.txt\"\n",
    "    ann_fn = p_tr / f\"{fn.name}\"\n",
    "    txt, sents = pre_processing(txt_fn, MIMICIII_PATTERN, max_len=256)\n",
    "    e2i, ens, relations, evns, attrs = load_annotation_brat(ann_fn)\n",
    "    i2e = {v: k for k, v in e2i.items()}\n",
    "    nsents, sent_bound = generate_BIO(sents, ens, file_id=\"\", no_overlap=False, record_pos=True)\n",
    "    nnsents = [w for sent in nsents for w in sent]\n",
    "    mappings = create_entity_to_sent_mapping(nnsents, ens, i2e, fn.name)\n",
    "\n",
    "    num_sents = len(nsents)\n",
    "    sent_ids = set(range(num_sents))\n",
    "    sent_with_drugs = set()\n",
    "\n",
    "    head_list = []\n",
    "    tail_ann_idx_mappings = defaultdict(list)\n",
    "    relation_ann_idx_mappings = defaultdict(list)\n",
    "\n",
    "\n",
    "    # sentence with entities\n",
    "    for en in ens:\n",
    "        entype = en[1]\n",
    "        ann_id = en[-1]\n",
    "        en_offset = en[2]\n",
    "        type_id = entity_id[entype]\n",
    "        text = en[0]\n",
    "        s_idx, e_idx = mappings[en[-1]]\n",
    "        word_info1 = nnsents[s_idx]\n",
    "        word_info2 = nnsents[e_idx]\n",
    "        \n",
    "        sent_idx1 = word_info1[3][0]\n",
    "        sent_idx2 = word_info2[3][0]\n",
    "        sent_with_drugs.add(sent_idx1)\n",
    "        \n",
    "        try:\n",
    "            sent_text = get_sent(nsents, sent_idx1, sent_idx2)\n",
    "            context = \" \".join([e[0] for e in sent_text])\n",
    "            start = word_info1[3][1]\n",
    "            end = word_info2[3][1]\n",
    "            start_end = f\"{start};{end}\"\n",
    "        except Exception as ex:\n",
    "            print(fn.name)\n",
    "            print(word_info1, word_info2)\n",
    "            print(context)\n",
    "            print(start, end, start_end)\n",
    "            print(entype, type_id)\n",
    "            \n",
    "            # key will be sent_id, type\n",
    "            # data will be tuple (context, entype, type_id, start, end, start_end)\n",
    "        if entype in head_entity:\n",
    "            head_list.append(\n",
    "                [sent_idx1, text, context, entype, en_offset, start, end, start_end, ann_id])\n",
    "        else:\n",
    "            tail_ann_idx_mappings[ann_id].append((sent_idx1, text, context, type_id, start, end, start_end))\n",
    "\n",
    "    for r in relations:\n",
    "        head_id = r[-1].split(\":\")[1]\n",
    "        tail_ent = r[0].split('-')[0]\n",
    "        tail_id = r[1].split(\":\")[1]\n",
    "\n",
    "        relation_ann_idx_mappings[(head_id, tail_ent)].append(tail_id)\n",
    "    \n",
    "    for head_ent in head_list:\n",
    "        head_sent_id = head_ent[0]\n",
    "        head_text = head_ent[1]\n",
    "        len_head_sent = len(nsents[head_sent_id])\n",
    "        file_id = f\"{fn.stem}\"\n",
    "        head_sent_context = \" \".join(e[0] for e in nsents[head_sent_id])\n",
    "        head_ann_id = head_ent[-1]\n",
    "        head_entity_type = head_ent[3]\n",
    "        head_offset_s, head_offset_e = head_ent[4]\n",
    "      \n",
    "\n",
    "        for k, v in attribute_query_template.items():\n",
    "            tid = attribute_id[k]\n",
    "            if (head_ann_id, k) in relation_ann_idx_mappings:\n",
    "                tail_ann_ids = relation_ann_idx_mappings[(head_ann_id, k)]\n",
    "\n",
    "                same_sent_start_positions = []\n",
    "                same_sent_end_positions = []\n",
    "                same_sent_span_positions = []\n",
    "\n",
    "                tail_sent_id_to_positions = defaultdict(lambda: ([], [], []))  # map from tail_sent_id to positions list\n",
    "                \n",
    "                for tail_ann_id in tail_ann_ids:\n",
    "                    tail_sent_id = tail_ann_idx_mappings[tail_ann_id][0][0]\n",
    "                    tail_sent_context = tail_ann_idx_mappings[tail_ann_id][0][2]\n",
    "\n",
    "                    s, e, se = tail_ann_idx_mappings[tail_ann_id][0][-3], tail_ann_idx_mappings[tail_ann_id][0][-2], tail_ann_idx_mappings[tail_ann_id][0][-1]\n",
    "\n",
    "                    if tail_sent_id == head_sent_id:\n",
    "                        same_sent_start_positions.append(s)\n",
    "                        same_sent_end_positions.append(e)\n",
    "                        same_sent_span_positions.append(se)\n",
    "                    else:\n",
    "                        s = s + (len_head_sent if tail_sent_id > head_sent_id else 0)\n",
    "                        e = e + (len_head_sent if tail_sent_id > head_sent_id else 0)\n",
    "                        se = f\"{s};{e}\"\n",
    "                        tail_sent_id_to_positions[tail_sent_id][0].append(s)\n",
    "                        tail_sent_id_to_positions[tail_sent_id][1].append(e)\n",
    "                        tail_sent_id_to_positions[tail_sent_id][2].append(se)\n",
    "                        tail_sent_context = tail_sent_context + ' ' + head_sent_context if tail_sent_id < head_sent_id else head_sent_context + ' ' + tail_sent_context\n",
    "\n",
    "                # Append data for the entities in the same sentence\n",
    "                if same_sent_start_positions:\n",
    "                    d = {\n",
    "                        \"context\": head_sent_context,\n",
    "                        \"end_position\": same_sent_end_positions,\n",
    "                        \"head_entity\": [head_entity_type, head_text, head_offset_s, head_offset_e],\n",
    "                        \"entity_label\": k,\n",
    "                        \"impossible\": False,\n",
    "                        \"qas_id\": f\"{file_id}.{head_sent_id}.{head_sent_id}.{tid}\",\n",
    "                        \"query\": v.format(head_text),\n",
    "                        \"span_position\": same_sent_span_positions,\n",
    "                        \"start_position\": same_sent_start_positions\n",
    "                    }\n",
    "                    training_data.append(d)\n",
    "                \n",
    "                # Append data for the entities in different sentences\n",
    "                for tail_sent_id, positions in tail_sent_id_to_positions.items():\n",
    "                    tail_sent_context = \" \".join(e[0] for e in nsents[tail_sent_id])\n",
    "                    d = {\n",
    "                        \"context\": tail_sent_context + ' ' + head_sent_context if tail_sent_id < head_sent_id else head_sent_context + ' ' + tail_sent_context,\n",
    "                        \"end_position\": positions[1],\n",
    "                        \"head_entity\": [head_entity_type, head_text, head_offset_s, head_offset_e],\n",
    "                        \"entity_label\": k,\n",
    "                        \"impossible\": False,\n",
    "                        \"qas_id\": f\"{file_id}.{head_sent_id}.{tail_sent_id}.{tid}\",\n",
    "                        \"query\": v.format(head_text),\n",
    "                        \"span_position\": positions[2],\n",
    "                        \"start_position\": positions[0]\n",
    "                    }\n",
    "                    training_data.append(d)\n",
    "            else:\n",
    "                # 'else' condition (i.e., 'head' entity does not have any 'tail' entity)\n",
    "                d = {\n",
    "                    \"context\": head_sent_context,\n",
    "                    \"end_position\": [],\n",
    "                    \"head_entity\": [head_entity_type, head_text, head_offset_s, head_offset_e],\n",
    "                    \"entity_label\": k,\n",
    "                    \"impossible\": True,\n",
    "                    \"qas_id\": f\"{file_id}.{head_sent_id}.{head_sent_id}.{tid}\",\n",
    "                    \"query\": v.format(head_text),\n",
    "                    \"span_position\": [],\n",
    "                    \"start_position\": []\n",
    "                }\n",
    "                training_data.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pout = Path(\"/data/datasets/cheng/ClinicalTransformerMRC/2018n2c2/dataset/mrc_relation\")\n",
    "pout.mkdir(parents=True, exist_ok=True)\n",
    "to_json(training_data, pout, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dev data\n",
    "#RE task\n",
    "dev_data = []\n",
    "\n",
    "for i, fn in enumerate(p_dev.glob(\"*.ann\")):\n",
    "\n",
    "    txt_fn = p_dev/ f\"{fn.stem}.txt\"\n",
    "    ann_fn = p_dev / f\"{fn.name}\"\n",
    "    txt, sents = pre_processing(txt_fn, MIMICIII_PATTERN, max_len=256)\n",
    "    e2i, ens, relations, evns, attrs = load_annotation_brat(ann_fn)\n",
    "    i2e = {v: k for k, v in e2i.items()}\n",
    "    nsents, sent_bound = generate_BIO(sents, ens, file_id=\"\", no_overlap=False, record_pos=True)\n",
    "    nnsents = [w for sent in nsents for w in sent]\n",
    "    mappings = create_entity_to_sent_mapping(nnsents, ens, i2e, fn.name)\n",
    "\n",
    "    num_sents = len(nsents)\n",
    "    sent_ids = set(range(num_sents))\n",
    "    sent_with_drugs = set()\n",
    "\n",
    "    head_list = []\n",
    "    tail_ann_idx_mappings = defaultdict(list)\n",
    "    relation_ann_idx_mappings = defaultdict(list)\n",
    "\n",
    "\n",
    "    # sentence with entities\n",
    "    for en in ens:\n",
    "        entype = en[1]\n",
    "        ann_id = en[-1]\n",
    "        en_offset = en[2]\n",
    "        type_id = entity_id[entype]\n",
    "        text = en[0]\n",
    "        s_idx, e_idx = mappings[en[-1]]\n",
    "        word_info1 = nnsents[s_idx]\n",
    "        word_info2 = nnsents[e_idx]\n",
    "        \n",
    "        sent_idx1 = word_info1[3][0]\n",
    "        sent_idx2 = word_info2[3][0]\n",
    "        sent_with_drugs.add(sent_idx1)\n",
    "        \n",
    "        try:\n",
    "            sent_text = get_sent(nsents, sent_idx1, sent_idx2)\n",
    "            context = \" \".join([e[0] for e in sent_text])\n",
    "            start = word_info1[3][1]\n",
    "            end = word_info2[3][1]\n",
    "            start_end = f\"{start};{end}\"\n",
    "        except Exception as ex:\n",
    "            print(fn.name)\n",
    "            print(word_info1, word_info2)\n",
    "            print(context)\n",
    "            print(start, end, start_end)\n",
    "            print(entype, type_id)\n",
    "            \n",
    "            # key will be sent_id, type\n",
    "            # data will be tuple (context, entype, type_id, start, end, start_end)\n",
    "        if entype in head_entity:\n",
    "            head_list.append(\n",
    "                [sent_idx1, text, context, entype, en_offset, start, end, start_end, ann_id])\n",
    "        else:\n",
    "            tail_ann_idx_mappings[ann_id].append((sent_idx1, text, context, type_id, start, end, start_end))\n",
    "\n",
    "    for r in relations:\n",
    "        head_id = r[-1].split(\":\")[1]\n",
    "        tail_ent = r[0].split('-')[0]\n",
    "        tail_id = r[1].split(\":\")[1]\n",
    "\n",
    "        relation_ann_idx_mappings[(head_id, tail_ent)].append(tail_id)\n",
    "    \n",
    "    for head_ent in head_list:\n",
    "        head_sent_id = head_ent[0]\n",
    "        head_text = head_ent[1]\n",
    "        len_head_sent = len(nsents[head_sent_id])\n",
    "        file_id = f\"{fn.stem}\"\n",
    "        head_sent_context = \" \".join(e[0] for e in nsents[head_sent_id])\n",
    "        head_ann_id = head_ent[-1]\n",
    "        head_entity_type = head_ent[3]\n",
    "        head_offset_s, head_offset_e = head_ent[4]\n",
    "      \n",
    "\n",
    "        for k, v in attribute_query_template.items():\n",
    "            tid = attribute_id[k]\n",
    "            if (head_ann_id, k) in relation_ann_idx_mappings:\n",
    "                tail_ann_ids = relation_ann_idx_mappings[(head_ann_id, k)]\n",
    "\n",
    "                same_sent_start_positions = []\n",
    "                same_sent_end_positions = []\n",
    "                same_sent_span_positions = []\n",
    "\n",
    "                tail_sent_id_to_positions = defaultdict(lambda: ([], [], []))  # map from tail_sent_id to positions list\n",
    "                \n",
    "                for tail_ann_id in tail_ann_ids:\n",
    "                    tail_sent_id = tail_ann_idx_mappings[tail_ann_id][0][0]\n",
    "                    tail_sent_context = tail_ann_idx_mappings[tail_ann_id][0][2]\n",
    "\n",
    "                    s, e, se = tail_ann_idx_mappings[tail_ann_id][0][-3], tail_ann_idx_mappings[tail_ann_id][0][-2], tail_ann_idx_mappings[tail_ann_id][0][-1]\n",
    "\n",
    "                    if tail_sent_id == head_sent_id:\n",
    "                        same_sent_start_positions.append(s)\n",
    "                        same_sent_end_positions.append(e)\n",
    "                        same_sent_span_positions.append(se)\n",
    "                    else:\n",
    "                        s = s + (len_head_sent if tail_sent_id > head_sent_id else 0)\n",
    "                        e = e + (len_head_sent if tail_sent_id > head_sent_id else 0)\n",
    "                        se = f\"{s};{e}\"\n",
    "                        tail_sent_id_to_positions[tail_sent_id][0].append(s)\n",
    "                        tail_sent_id_to_positions[tail_sent_id][1].append(e)\n",
    "                        tail_sent_id_to_positions[tail_sent_id][2].append(se)\n",
    "                        tail_sent_context = tail_sent_context + ' ' + head_sent_context if tail_sent_id < head_sent_id else head_sent_context + ' ' + tail_sent_context\n",
    "\n",
    "                # Append data for the entities in the same sentence\n",
    "                if same_sent_start_positions:\n",
    "                    d = {\n",
    "                        \"context\": head_sent_context,\n",
    "                        \"end_position\": same_sent_end_positions,\n",
    "                        \"head_entity\": [head_entity_type, head_text, head_offset_s, head_offset_e],\n",
    "                        \"entity_label\": k,\n",
    "                        \"impossible\": False,\n",
    "                        \"qas_id\": f\"{file_id}.{head_sent_id}.{head_sent_id}.{tid}\",\n",
    "                        \"query\": v.format(head_text),\n",
    "                        \"span_position\": same_sent_span_positions,\n",
    "                        \"start_position\": same_sent_start_positions\n",
    "                    }\n",
    "                    dev_data.append(d)\n",
    "                \n",
    "                # Append data for the entities in different sentences\n",
    "                for tail_sent_id, positions in tail_sent_id_to_positions.items():\n",
    "                    tail_sent_context = \" \".join(e[0] for e in nsents[tail_sent_id])\n",
    "                    d = {\n",
    "                        \"context\": tail_sent_context + ' ' + head_sent_context if tail_sent_id < head_sent_id else head_sent_context + ' ' + tail_sent_context,\n",
    "                        \"end_position\": positions[1],\n",
    "                        \"head_entity\": [head_entity_type, head_text, head_offset_s, head_offset_e],\n",
    "                        \"entity_label\": k,\n",
    "                        \"impossible\": False,\n",
    "                        \"qas_id\": f\"{file_id}.{head_sent_id}.{tail_sent_id}.{tid}\",\n",
    "                        \"query\": v.format(head_text),\n",
    "                        \"span_position\": positions[2],\n",
    "                        \"start_position\": positions[0]\n",
    "                    }\n",
    "                    dev_data.append(d)\n",
    "            else:\n",
    "                # 'else' condition (i.e., 'head' entity does not have any 'tail' entity)\n",
    "                d = {\n",
    "                    \"context\": head_sent_context,\n",
    "                    \"end_position\": [],\n",
    "                    \"head_entity\": [head_entity_type, head_text, head_offset_s, head_offset_e],\n",
    "                    \"entity_label\": k,\n",
    "                    \"impossible\": True,\n",
    "                    \"qas_id\": f\"{file_id}.{head_sent_id}.{head_sent_id}.{tid}\",\n",
    "                    \"query\": v.format(head_text),\n",
    "                    \"span_position\": [],\n",
    "                    \"start_position\": []\n",
    "                }\n",
    "                dev_data.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pout = Path(\"../2018_n2c2/data/mrc_relation\")\n",
    "pout.mkdir(parents=True, exist_ok=True)\n",
    "to_json(dev_data, pout, \"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create test data\n",
    "#RE task\n",
    "test_data = []\n",
    "\n",
    "for i, fn in enumerate(p_dev.glob(\"*.ann\")):\n",
    "\n",
    "    txt_fn = p_dev/ f\"{fn.stem}.txt\"\n",
    "    ann_fn = p_dev / f\"{fn.name}\"\n",
    "    txt, sents = pre_processing(txt_fn, MIMICIII_PATTERN, max_len=256)\n",
    "    e2i, ens, relations, evns, attrs = load_annotation_brat(ann_fn)\n",
    "    i2e = {v: k for k, v in e2i.items()}\n",
    "    nsents, sent_bound = generate_BIO(sents, ens, file_id=\"\", no_overlap=False, record_pos=True)\n",
    "    nnsents = [w for sent in nsents for w in sent]\n",
    "    mappings = create_entity_to_sent_mapping(nnsents, ens, i2e, fn.name)\n",
    "\n",
    "    num_sents = len(nsents)\n",
    "    sent_ids = set(range(num_sents))\n",
    "    sent_with_drugs = set()\n",
    "\n",
    "    head_list = []\n",
    "    tail_ann_idx_mappings = defaultdict(list)\n",
    "    relation_ann_idx_mappings = defaultdict(list)\n",
    "\n",
    "\n",
    "    # sentence with entities\n",
    "    for en in ens:\n",
    "        entype = en[1]\n",
    "        ann_id = en[-1]\n",
    "        en_offset = en[2]\n",
    "        type_id = entity_id[entype]\n",
    "        text = en[0]\n",
    "        s_idx, e_idx = mappings[en[-1]]\n",
    "        word_info1 = nnsents[s_idx]\n",
    "        word_info2 = nnsents[e_idx]\n",
    "        \n",
    "        sent_idx1 = word_info1[3][0]\n",
    "        sent_idx2 = word_info2[3][0]\n",
    "        sent_with_drugs.add(sent_idx1)\n",
    "        \n",
    "        try:\n",
    "            sent_text = get_sent(nsents, sent_idx1, sent_idx2)\n",
    "            context = \" \".join([e[0] for e in sent_text])\n",
    "            start = word_info1[3][1]\n",
    "            end = word_info2[3][1]\n",
    "            start_end = f\"{start};{end}\"\n",
    "        except Exception as ex:\n",
    "            print(fn.name)\n",
    "            print(word_info1, word_info2)\n",
    "            print(context)\n",
    "            print(start, end, start_end)\n",
    "            print(entype, type_id)\n",
    "            \n",
    "            # key will be sent_id, type\n",
    "            # data will be tuple (context, entype, type_id, start, end, start_end)\n",
    "        if entype in head_entity:\n",
    "            head_list.append(\n",
    "                [sent_idx1, text, context, entype, en_offset, start, end, start_end, ann_id])\n",
    "        else:\n",
    "            tail_ann_idx_mappings[ann_id].append((sent_idx1, text, context, type_id, start, end, start_end))\n",
    "\n",
    "    for r in relations:\n",
    "        head_id = r[-1].split(\":\")[1]\n",
    "        tail_ent = r[0].split('-')[0]\n",
    "        tail_id = r[1].split(\":\")[1]\n",
    "\n",
    "        relation_ann_idx_mappings[(head_id, tail_ent)].append(tail_id)\n",
    "    \n",
    "    for head_ent in head_list:\n",
    "        head_sent_id = head_ent[0]\n",
    "        head_text = head_ent[1]\n",
    "        len_head_sent = len(nsents[head_sent_id])\n",
    "        file_id = f\"{fn.stem}\"\n",
    "        head_sent_context = \" \".join(e[0] for e in nsents[head_sent_id])\n",
    "        head_ann_id = head_ent[-1]\n",
    "        head_entity_type = head_ent[3]\n",
    "        head_offset_s, head_offset_e = head_ent[4]\n",
    "      \n",
    "\n",
    "        for k, v in attribute_query_template.items():\n",
    "            tid = attribute_id[k]\n",
    "            if (head_ann_id, k) in relation_ann_idx_mappings:\n",
    "                tail_ann_ids = relation_ann_idx_mappings[(head_ann_id, k)]\n",
    "\n",
    "                same_sent_start_positions = []\n",
    "                same_sent_end_positions = []\n",
    "                same_sent_span_positions = []\n",
    "\n",
    "                tail_sent_id_to_positions = defaultdict(lambda: ([], [], []))  # map from tail_sent_id to positions list\n",
    "                \n",
    "                for tail_ann_id in tail_ann_ids:\n",
    "                    tail_sent_id = tail_ann_idx_mappings[tail_ann_id][0][0]\n",
    "                    tail_sent_context = tail_ann_idx_mappings[tail_ann_id][0][2]\n",
    "\n",
    "                    s, e, se = tail_ann_idx_mappings[tail_ann_id][0][-3], tail_ann_idx_mappings[tail_ann_id][0][-2], tail_ann_idx_mappings[tail_ann_id][0][-1]\n",
    "\n",
    "                    if tail_sent_id == head_sent_id:\n",
    "                        same_sent_start_positions.append(s)\n",
    "                        same_sent_end_positions.append(e)\n",
    "                        same_sent_span_positions.append(se)\n",
    "                    else:\n",
    "                        s = s + (len_head_sent if tail_sent_id > head_sent_id else 0)\n",
    "                        e = e + (len_head_sent if tail_sent_id > head_sent_id else 0)\n",
    "                        se = f\"{s};{e}\"\n",
    "                        tail_sent_id_to_positions[tail_sent_id][0].append(s)\n",
    "                        tail_sent_id_to_positions[tail_sent_id][1].append(e)\n",
    "                        tail_sent_id_to_positions[tail_sent_id][2].append(se)\n",
    "                        tail_sent_context = tail_sent_context + ' ' + head_sent_context if tail_sent_id < head_sent_id else head_sent_context + ' ' + tail_sent_context\n",
    "\n",
    "                # Append data for the entities in the same sentence\n",
    "                if same_sent_start_positions:\n",
    "                    d = {\n",
    "                        \"context\": head_sent_context,\n",
    "                        \"end_position\": [],\n",
    "                        \"head_entity\": [head_entity_type, head_text, head_offset_s, head_offset_e],\n",
    "                        \"entity_label\": k,\n",
    "                        \"impossible\": True,\n",
    "                        \"qas_id\": f\"{file_id}.{head_sent_id}.{head_sent_id}.{tid}\",\n",
    "                        \"query\": v.format(head_text),\n",
    "                        \"span_position\": [],\n",
    "                        \"start_position\": []\n",
    "                    }\n",
    "                    test_data.append(d)\n",
    "                \n",
    "                # Append data for the entities in different sentences\n",
    "                for tail_sent_id, positions in tail_sent_id_to_positions.items():\n",
    "                    tail_sent_context = \" \".join(e[0] for e in nsents[tail_sent_id])\n",
    "                    d = {\n",
    "                        \"context\": tail_sent_context + ' ' + head_sent_context if tail_sent_id < head_sent_id else head_sent_context + ' ' + tail_sent_context,\n",
    "                        \"end_position\": [],\n",
    "                        \"head_entity\": [head_entity_type, head_text, head_offset_s, head_offset_e],\n",
    "                        \"entity_label\": k,\n",
    "                        \"impossible\": True,\n",
    "                        \"qas_id\": f\"{file_id}.{head_sent_id}.{tail_sent_id}.{tid}\",\n",
    "                        \"query\": v.format(head_text),\n",
    "                        \"span_position\": [],\n",
    "                        \"start_position\": []\n",
    "                    }\n",
    "                    test_data.append(d)\n",
    "            else:\n",
    "                # 'else' condition (i.e., 'head' entity does not have any 'tail' entity)\n",
    "                d = {\n",
    "                    \"context\": head_sent_context,\n",
    "                    \"end_position\": [],\n",
    "                    \"head_entity\": [head_entity_type, head_text, head_offset_s, head_offset_e],\n",
    "                    \"entity_label\": k,\n",
    "                    \"impossible\": True,\n",
    "                    \"qas_id\": f\"{file_id}.{head_sent_id}.{head_sent_id}.{tid}\",\n",
    "                    \"query\": v.format(head_text),\n",
    "                    \"span_position\": [],\n",
    "                    \"start_position\": []\n",
    "                }\n",
    "                test_data.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pout = Path(\"../2018_n2c2/data/mrc_relation\")\n",
    "pout.mkdir(parents=True, exist_ok=True)\n",
    "to_json(test_data, pout, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_index_to_wordindex (token_start, token_end, tokens, ccontext, nnsents, sent_id):\n",
    "\n",
    "    corpora_records = ccontext.split()\n",
    "    word_2_char_mapping={}\n",
    "    char_cursor=0\n",
    "    for ind in range(len(corpora_records)):\n",
    "        if(len(corpora_records[ind])>0):#the last space will not be considered\n",
    "            start=char_cursor\n",
    "            end=char_cursor+len(corpora_records[ind])\n",
    "            word_2_char_mapping[ind]=[start,end]\n",
    "            char_cursor=char_cursor+len(corpora_records[ind])+1#consider the white-space length\n",
    "    # print(ccontext)\n",
    "    # print(word_2_char_mapping)\n",
    "    start_char_span=tokens.token_to_chars(token_start)\n",
    "    end_char_span=tokens.token_to_chars(token_end)\n",
    "    # print(start_char_span,end_char_span)\n",
    "    for each_word in word_2_char_mapping:\n",
    "        start = word_2_char_mapping[each_word][0]\n",
    "        end = word_2_char_mapping[each_word][1]\n",
    "        \n",
    "        if(start_char_span[0]>=start and start_char_span[1]<=end):\n",
    "            s_char = each_word\n",
    "\n",
    "        if(end_char_span[0]>=start and end_char_span[1]<=end):\n",
    "            e_char = each_word\n",
    "            \n",
    "    print(sent_id, s_char, e_char, ccontext)\n",
    "    for i, sent in enumerate(nnsents):\n",
    "        if (sent_id, s_char) == sent[3]:\n",
    "            s_offset = sent[1][0]\n",
    "            \n",
    "        if (sent_id, e_char) == sent[3]:\n",
    "            e_offset = sent[1][1]\n",
    "            \n",
    "    return s_offset, e_offset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the output to brat format\n",
    "#NER task\n",
    "import os\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "\n",
    "# Define paths\n",
    "data_root = Path(\"../2018_n2c2\")\n",
    "test_data_path = data_root / \"test_data\"\n",
    "mrc_data_file = data_root / \"mrc_entity/mrc-ner.test\"\n",
    "result_path = data_root / \"exp/ner/pred\"\n",
    "entity_model_name = 'bert-large-cased'\n",
    "entity_prediction_file = result_path / f\"{entity_model_name}.json\"\n",
    "output_path = data_root / \"exp/ner/results/\"\n",
    "bert_model_path = \"../bert-large-cased\"\n",
    "\n",
    "vocab_file = os.path.join(bert_model_path, \"vocab.txt\")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertWordPieceTokenizer(vocab_file)\n",
    "\n",
    "# Load entity predictions\n",
    "with open(entity_prediction_file, \"r\") as f:\n",
    "    entity_predictions = json.load(f)\n",
    "\n",
    "# Load MRC formart entity data\n",
    "with open(mrc_data_file, \"r\") as f:\n",
    "    mrc_entity_data = json.load(f)\n",
    "\n",
    "# Define output format\n",
    "BRAT_TEMPLATE_T = \"{}\\t{} {} {}\\t{}\"\n",
    "output_template = BRAT_TEMPLATE_T\n",
    "\n",
    "output_file_suffix = 'ann'\n",
    "\n",
    "for i, fn in enumerate(test_data_path.glob(\"*.txt\")):\n",
    "    \n",
    "    txt_fn = test_data_path / f\"{fn.name}\"\n",
    "    txt_data = open(txt_fn,'r').read()\n",
    "    txt, sents = pre_processing(txt_fn, MIMICIII_PATTERN, max_len=256)\n",
    "    output_fn = output_path / \"{}.{}\".format(fn.stem, output_file_suffix)\n",
    "    \n",
    "    nsents, sent_bound = generate_BIO(sents, [], file_id=\"\", no_overlap=False, record_pos=True)\n",
    "    nnsents = [w for sent in nsents for w in sent]\n",
    "\n",
    "    num_sents = len(nsents)\n",
    "    sent_ids = set(range(num_sents))\n",
    "    \n",
    "    # Match entity predictions for each txt file\n",
    "    matched_entity_predictions = []\n",
    "\n",
    "    \n",
    "    for i, pred in enumerate(entity_predictions):\n",
    "        if int(str(pred['sample_idx'])) == int(fn.stem):\n",
    "            matched_entity_predictions.append([i, pred])\n",
    "    \n",
    "    # Map token-level predictions back to original text and format in BRAT\n",
    "    brat_entities = []\n",
    "\n",
    "    for prediction_idex, entity_prediction in matched_entity_predictions:\n",
    "        sent_id= int(str(entity_prediction['head_sent_idx'][0]))\n",
    "    \n",
    "        context = mrc_entity_data[prediction_idex]['context']\n",
    "        query = mrc_entity_data[prediction_idex]['query']\n",
    "        query_tokens = tokenizer.encode(query, add_special_tokens=False)\n",
    "        context_tokens = tokenizer.encode(context, add_special_tokens=False)\n",
    "\n",
    "        \n",
    "        for entity in entity_prediction['en']:\n",
    "            token_start = entity[0]- len(query_tokens) - 2\n",
    "            token_end = entity[1]- len(query_tokens) - 3\n",
    "            print(entity)\n",
    "            entity_start, entity_end = remap_index_to_wordindex(token_start, token_end, context_tokens, context, nnsents, sent_id)\n",
    "            ent_type = entity[3]\n",
    "            entity_word = txt_data[entity_start: entity_end].replace(\"\\n\", \" \")\n",
    "            brat_entities.append((ent_type, entity_start, entity_end, entity_word))\n",
    "            \n",
    "# Write BRAT-formatted entities to output file\n",
    "    with open(output_fn, \"w\") as f:\n",
    "        for entity in brat_entities:\n",
    "            f.write(output_template.format(*entity))\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the output to brat format\n",
    "#RE task\n",
    "import os\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "data_root = Path(\"../2018_n2c2\")\n",
    "test_data_path = data_root / \"test_data\"\n",
    "mrc_data_file = data_root / \"mrc_relation/mrc-ner.test\"\n",
    "result_path = data_root / \"exp/re/pred\"\n",
    "relation_model_name = 'bert-large-cased'\n",
    "relation_prediction_file = result_path / f\"{entity_model_name}.json\"\n",
    "output_path = data_root / \"exp/ner/results/\"\n",
    "bert_model_path = \"../bert-large-cased\"\n",
    "vocab_file = os.path.join(bert_model_path, \"vocab.txt\")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertWordPieceTokenizer(vocab_file)\n",
    "\n",
    "# Load relation predictions\n",
    "with open(relation_prediction_file, \"r\") as f:\n",
    "    relation_predictions = json.load(f)\n",
    "\n",
    "# Load MRC formart entity data\n",
    "with open(mrc_data_file, \"r\") as f:\n",
    "    mrc_relation_data = json.load(f)\n",
    "\n",
    "# Define output format\n",
    "BRAT_TEMPLATE_T = \"{}\\t{} {} {}\\t{}\"\n",
    "output_template_t = BRAT_TEMPLATE_T\n",
    "BRAT_TEMPLATE_R = \"{}\\t{}-{} Arg1:{} Arg2:{}\"\n",
    "output_template_r = BRAT_TEMPLATE_R\n",
    "\n",
    "file_suffix = 'ann'\n",
    "\n",
    "for i, fn in enumerate(test_data_path.glob(\"*.txt\")):\n",
    "    txt_fn = test_data_path / f\"{fn.name}\"\n",
    "    txt_data = open(txt_fn,'r').read()\n",
    "    \n",
    "    txt, sents = pre_processing(txt_fn, MIMICIII_PATTERN, max_len=256)\n",
    "    output_fn = output_path / \"{}.{}\".format(fn.stem, file_suffix)\n",
    "    \n",
    "    nsents, sent_bound = generate_BIO(sents, [], file_id=\"\", no_overlap=False, record_pos=True)\n",
    "    nnsents = [w for sent in nsents for w in sent]\n",
    "\n",
    "    num_sents = len(nsents)\n",
    "    sent_ids = set(range(num_sents))\n",
    "  \n",
    "    # Match entity predictions for each txt file\n",
    "    matched_relation_predictions = []\n",
    "\n",
    "    event_prediction = defaultdict(list)\n",
    "     \n",
    "    for i, pred in enumerate(relation_predictions):\n",
    "         if int(str(pred['sample_idx'])) == int(fn.stem):\n",
    "            matched_relation_predictions.append([i, pred])\n",
    "    \n",
    "\n",
    "    for prediction_idex, relation_prediction in matched_relation_predictions:\n",
    "    \n",
    "        head_sent_id = int(relation_prediction['head_sent_idx'][0])\n",
    "        tail_sent_id = int(relation_prediction['tail_sent_idx'][0])\n",
    "        \n",
    "        head_type = mrc_relation_data[prediction_idex]['head_entity'][0]\n",
    "        head_text = mrc_relation_data[prediction_idex]['head_entity'][1]\n",
    "        head_offset_s = mrc_relation_data[prediction_idex]['head_entity'][2]\n",
    "        head_offset_e = mrc_relation_data[prediction_idex]['head_entity'][3]\n",
    "        \n",
    "        context = mrc_relation_data[prediction_idex]['context']\n",
    "        query = mrc_relation_data[prediction_idex]['query']\n",
    "        query_tokens = tokenizer.encode(query, add_special_tokens=False)\n",
    "        context_tokens = tokenizer.encode(context, add_special_tokens=False)\n",
    "\n",
    "        \n",
    "        \n",
    "        for tail_entity in relation_prediction['en']:\n",
    "            ent_type = tail_entity[3]\n",
    "            if head_sent_id == tail_sent_id:\n",
    "                token_start = tail_entity[0] - len(query_tokens) - 2\n",
    "                token_end = tail_entity[1] - len(query_tokens) - 3\n",
    "                \n",
    "            \n",
    "                entity_start, entity_end = remap_index_to_wordindex (token_start, token_end, context_tokens, context, nnsents, head_sent_id)\n",
    "            \n",
    "            if head_sent_id < tail_sent_id:\n",
    "                head_sent = \" \".join(e[0] for e in nsents[head_sent_id])\n",
    "                head_sent_tokens = tokenizer.encode(head_sent, add_special_tokens=False)\n",
    "                token_start = tail_entity[0] - len(query_tokens) - len(head_sent_tokens) - 2\n",
    "                token_end = tail_entity[1] - len(query_tokens) - len(head_sent_tokens) - 3\n",
    "                tail_sent = \" \".join(e[0] for e in nsents[tail_sent_id])\n",
    "                tail_sent_tokens = tokenizer.encode(tail_sent, add_special_tokens=False)\n",
    "                entity_start, endtity_end = remap_index_to_wordindex (token_start, token_end, tail_sent_tokens, tail_sent, nnsents, tail_sent_id) \n",
    "\n",
    "            if head_sent_id > tail_sent_id:\n",
    "                tail_sent = \" \".join(e[0] for e in nsents[tail_sent_id])\n",
    "                tail_sent_tokens = tokenizer.encode(tail_sent, add_special_tokens=False)\n",
    "                token_start = tail_entity[0] - len(query_tokens) - 2\n",
    "                token_end = tail_entity[1] - len(query_tokens) - 3\n",
    "                entity_start, entity_end = remap_index_to_wordindex (token_start, token_end, tail_sent_tokens, tail_sent, nnsents, tail_sent_id)\n",
    "\n",
    "            entity_word = txt_data[entity_start:entity_end].replace(\"\\n\", \" \")\n",
    "            \n",
    "            event_prediction[(head_text, head_offset_s, head_offset_e)] .append((ent_type, entity_start, entity_end, entity_word))\n",
    "    \n",
    "    output_tr = []\n",
    "    i = 1\n",
    "    k = 1\n",
    "    m = 1\n",
    "    for event in list(event_prediction):\n",
    "        head_ent = event\n",
    "\n",
    "        text_h, head_s, head_e  = head_ent\n",
    "        formatted_output_head = output_template_t.format(\"T{}\".format(i), 'Drug', head_s, head_e, text_h)\n",
    "        output_tr.append (formatted_output_head)\n",
    "        i= i + 1 \n",
    "\n",
    "        for attributes in event_prediction[event]:\n",
    "            att_type, tail_s, tail_e, text_t  = attributes\n",
    "            formatted_output_att = output_template_t.format(\"T{}\".format(i), att_type, tail_s, tail_e, text_t)\n",
    "            formatted_output_rel = output_template_r.format(\"R{}\".format(m), att_type, 'Drug', \"T{}\".format(i), \"T{}\".format(k))\n",
    "            output_tr.append(formatted_output_att)\n",
    "            output_tr.append(formatted_output_rel)\n",
    "            i = i + 1\n",
    "            m = m + 1\n",
    "        k = i\n",
    "\n",
    "    with open(output_fn, \"w\") as f:\n",
    "        formatted_output = \"\\n\".join(output_tr)\n",
    "        f.write(formatted_output)\n",
    "        f.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f678c47ef45de972fc7e03e8f6b770ee0fe5ec26f99d66888e951db52c49df0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
