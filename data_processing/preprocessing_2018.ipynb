{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "505"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_tr = Path(\"/data/datasets/cheng/mrc-for-ner-medical/2018_n2c2/2018n2c2_track2_training/\")\n",
    "p_dev = Path(\"/data/datasets/cheng/mrc-for-ner-medical/2018_n2c2/gold_standard_test/\")\n",
    "\n",
    "# p_tr = Path(\"/data/datasets/cheng/mrc-for-ner-medical/2018_n2c2/2018n2c2_track2_training/\")\n",
    "\n",
    "fids = []\n",
    "\n",
    "fids.extend([f for f in p_tr.glob(\"*.ann\")])\n",
    "fids.extend([f for f in p_dev.glob(\"*.ann\")])\n",
    "\n",
    "len(fids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path(\"/data/datasets/cheng/mrc-for-ner-medical/2018_n2c2/data/mimic\")\n",
    "p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "xx = [shutil.copyfile(fn, p/f\"{fn.name}\") for fn in fids]\n",
    "xx = [shutil.copyfile(fn.parent/f\"{fn.stem}.txt\", p/f\"{fn.stem}.txt\") for fn in fids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle as pkl\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import permutations, combinations\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# https://github.com/uf-hobi-informatics-lab/NLPreprocessing (git clone this repo to local)\n",
    "sys.path.append(\"./NLPreprocessing/\")\n",
    "sys.path.append(\"./NLPreprocessing/text_process\")\n",
    "from annotation2BIO import pre_processing, read_annotation_brat, generate_BIO\n",
    "MIMICIII_PATTERN = \"\"\n",
    "from sentence_tokenization import logger as l1\n",
    "from annotation2BIO import logger as l2\n",
    "l1.disabled = True\n",
    "l2.disabled = True\n",
    "\n",
    "def pkl_save(data, file):\n",
    "    with open(file, \"wb\") as f:\n",
    "        pkl.dump(data, f)\n",
    "\n",
    "        \n",
    "def pkl_load(file):\n",
    "    with open(file, \"rb\") as f:\n",
    "        data = pkl.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_text(ifn):\n",
    "    with open(ifn, \"r\") as f:\n",
    "        txt = f.read()\n",
    "    return txt\n",
    "\n",
    "\n",
    "def save_text(text, ofn):\n",
    "    with open(ofn, \"w\") as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_entity_to_sent_mapping(nnsents, entities, idx2e, fn):\n",
    "    loc_ens = []\n",
    "    \n",
    "    ll = len(nnsents)\n",
    "    mapping = defaultdict(list)\n",
    "    for idx, each in enumerate(entities):\n",
    "        en_label = idx2e[idx]\n",
    "        en_s = each[2][0]\n",
    "        en_e = each[2][1]\n",
    "        new_en = []\n",
    "        \n",
    "        i = 0\n",
    "        while i < ll and nnsents[i][1][0] < en_s:\n",
    "            i += 1\n",
    "        s_s = nnsents[i][1][0]\n",
    "        s_e = nnsents[i][1][1]\n",
    "\n",
    "        if en_s == s_s:\n",
    "            mapping[en_label].append(i)\n",
    "\n",
    "            while i < ll and s_e < en_e:\n",
    "                i += 1\n",
    "                s_e = nnsents[i][1][1]\n",
    "            if s_e == en_e:\n",
    "                 mapping[en_label].append(i)\n",
    "            else:\n",
    "                mapping[en_label].append(i)\n",
    "                print(fn)\n",
    "                print(\"last index not match \", each)\n",
    "        else:\n",
    "            mapping[en_label].append(i)\n",
    "            print(fn)\n",
    "            print(\"first index not match \", each)\n",
    "\n",
    "            while i < ll and s_e < en_e:\n",
    "                i += 1\n",
    "                s_e = nnsents[i][1][1]\n",
    "            if s_e == en_e:\n",
    "                 mapping[en_label].append(i)\n",
    "            else:\n",
    "                mapping[en_label].append(i)\n",
    "                print(fn)\n",
    "                print(\"last index not match \", each)\n",
    "    return mapping\n",
    "\n",
    "    \n",
    "def __ann_info(ann):\n",
    "    en_info = ann.split(\" \")\n",
    "    return en_info[0], int(en_info[1]), int(en_info[-1])\n",
    "\n",
    "\n",
    "def load_annotation_brat(ann_file, rep=False):\n",
    "    \"\"\"\n",
    "    load annotation data\n",
    "    entity_id2index_map -> {'T1': 0}\n",
    "    entites -> ('T1', 'anticoagulant medications', 'Drug', (1000, 1025))\n",
    "    relations -> ('Route-Drug', 'Arg1:T3', 'Arg2:T2')\n",
    "    \"\"\"\n",
    "    # map the entity id (e.g., T1) to its index in entities list\n",
    "    entity_id2index_map = dict()\n",
    "    entites = []\n",
    "    relations = []\n",
    "    events = []\n",
    "    attrs = []\n",
    "    \n",
    "    with open(ann_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            anns = line.split(\"\\t\")\n",
    "            ann_id = anns[0]\n",
    "            if ann_id.startswith(\"T\"):\n",
    "                # T1\tLivingStatus 25 30\tlives\n",
    "                entity_words = anns[-1]\n",
    "                t_type, offset_s, offset_e = __ann_info(anns[1])\n",
    "                entites.append((entity_words, t_type, (offset_s, offset_e), ann_id))\n",
    "                entity_id2index_map[ann_id] = len(entites) - 1\n",
    "\n",
    "            elif ann_id.startswith(\"A\"):\n",
    "                #  A1\tStatusTimeVal T2 current\n",
    "                att_type, env_id, att_val = anns[1].strip().split(\" \")\n",
    "                attrs.append((att_type, env_id, att_val))\n",
    "\n",
    "            elif ann_id.startswith(\"E\"):\n",
    "                # E2\tAlcohol:T3 Status:T4 Amount:T5 Frequency:T6 Type:T10\n",
    "                single_event = dict()\n",
    "                envs = anns[1].split(\" \")\n",
    "                trigger = envs[0].strip().split(\":\")\n",
    "                tas = []\n",
    "                for each in envs[1:]:\n",
    "                    tas.append(each.strip().split(\":\"))\n",
    "                \n",
    "                single_event[\"trigger\"] = trigger\n",
    "                single_event[\"events\"] = tas\n",
    "                    \n",
    "                events.append(single_event)\n",
    "            elif ann_id.startswith(\"R\"):\n",
    "                # R2\tStrength-Drug Arg1:T6 Arg2:T5\n",
    "                relation, tail, head = anns[1].split()[0], anns[1].split()[1], anns[1].split()[-1]\n",
    "                \n",
    "                relations.append((relation, tail, head))\n",
    "\n",
    "    return entity_id2index_map, entites, relations, events, attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "sample data:\n",
    "a list of dict as\n",
    "  {\n",
    "    \"context\": \"Germany 's representative to the European Union 's veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer .\",\n",
    "    \"end_position\": [\n",
    "      0,\n",
    "      23\n",
    "    ],\n",
    "    \"entity_label\": \"LOC\",\n",
    "    \"impossible\": false,\n",
    "    \"qas_id\": \"4.3\",\n",
    "    \"query\": \"location entities are the name of politically or geographically defined locations such as cities, provinces, countries, international regions, bodies of water, mountains, etc.\",\n",
    "    \"span_position\": [\n",
    "      \"0;0\",\n",
    "      \"23;23\"\n",
    "    ],\n",
    "    \"start_position\": [\n",
    "      0,\n",
    "      23\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"context\": \"EU rejects German call to boycott British lamb .\",\n",
    "    \"end_position\": [],\n",
    "    \"entity_label\": \"PER\",\n",
    "    \"impossible\": true,\n",
    "    \"qas_id\": \"0.2\",\n",
    "    \"query\": \"person entities are named persons or family.\",\n",
    "    \"span_position\": [],\n",
    "    \"start_position\": []\n",
    "  }, ...\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# create trigger template\n",
    "# make sure the label id in qas_id is consistent for each type of entity\n",
    "entity_query_template = {\n",
    "    \"Drug\": \"Find the drug events including names, brand names, and collective names.\",\n",
    "    \"Strength\": \"Find the strength events that are the amount of drug in a given dosage.\",\n",
    "    \"Form\": \"Find the form events that are the physical form of given drug or medication.\", \n",
    "    \"Dosage\": \"Find the dosage events that are the amount of a medication used in each administration.\", \n",
    "    \"Frequency\":\"Frequency indicates how often each dose of the medication should be taken\", \n",
    "    \"Route\": \"Find the frequency events that indicate how often each dose of the medication should be taken.\", \n",
    "    \"Duration\": \"Find the duration events that indicate how long the medication is to be administered.\", \n",
    "    \"Reason\": \"Find the reason events that are the medical reason for which the medication is given.\",\n",
    "    \"ADE\": \"Find the ADE events that are injuries resulting from a medical intervention related to drugs.\"\n",
    "}\n",
    "\n",
    "attribute_query_template = {\n",
    "    \"Strength\": \"What is the active ingredient amount of {}\",\n",
    "    \"Form\": \"What is the physical form of {}\", \n",
    "    \"Dosage\": \"What is the amount of {} taken\", \n",
    "    \"Frequency\":\"How often each dose of {} should be taken\", \n",
    "    \"Route\": \"What is the path of {} taken into the body\", \n",
    "    \"Duration\": \"How long to take {}\", \n",
    "    \"Reason\": \"What is the medical reason for giving {}\",\n",
    "    \"ADE\": \"What are the injuries resulting from the use of {}\"\n",
    "}\n",
    "\n",
    "entity_id = {\n",
    "    \"Drug\": 1, \"Strength\": 2, \"Form\": 3, \"Dosage\": 4, \"Frequency\": 5, \"Route\": 6, \"Duration\": 7, \"Reason\": 8, \"ADE\": 9\n",
    "}\n",
    "\n",
    "attribute_id = {\n",
    "    \"Strength\": 1, \"Form\": 2, \"Dosage\": 3, \"Frequency\": 4, \"Route\": 5, \"Duration\": 6, \"Reason\": 7, \"ADE\": 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent(sents, idx1, idx2):\n",
    "    if idx1 == idx2:\n",
    "        return sents[idx1]\n",
    "    elif idx1 == idx2 + 1:\n",
    "        raise Exception(f\"{idx1} {idx2} - entity not in the same sentence\")\n",
    "    else:\n",
    "        raise Exception(f\"{idx1} {idx2} - the entity has word spread in >2 sentences\")\n",
    "        \n",
    "        \n",
    "def to_json(data, p, fn=\"train\"):\n",
    "    import json\n",
    "    \n",
    "    ofn = p / f\"mrc-ner.{fn}\"\n",
    "    \n",
    "    with open(ofn, \"w\") as f:\n",
    "        json.dump(data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'entity_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_84746/3405186479.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mentype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0men\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mtype_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentity_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mentype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0ms_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmappings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'entity_id' is not defined"
     ]
    }
   ],
   "source": [
    "##single sentence as a sample\n",
    "\n",
    "training_data = []\n",
    "\n",
    "\n",
    "for i, fn in enumerate(p_tr.glob(\"*.ann\")):\n",
    "\n",
    "    txt_fn = p_tr / f\"{fn.stem}.txt\"\n",
    "    ann_fn = p_tr / f\"{fn.name}\"\n",
    "    txt, sents = pre_processing(txt_fn, MIMICIII_PATTERN, max_len=256)\n",
    "    e2i, ens, relations, evns, attrs = load_annotation_brat(ann_fn)\n",
    "    i2e = {v: k for k, v in e2i.items()}\n",
    "    nsents, sent_bound = generate_BIO(sents, ens, file_id=\"\", no_overlap=False, record_pos=True)\n",
    "    nnsents = [w for sent in nsents for w in sent]\n",
    "    mappings = create_entity_to_sent_mapping(nnsents, ens, i2e, fn.name)\n",
    "\n",
    "    num_sents = len(nsents)\n",
    "    sent_ids = set(range(num_sents))\n",
    "    sent_with_entities = set()\n",
    "\n",
    "    entity_sent_idx_mappings = defaultdict(list)\n",
    "\n",
    "    # sentence with entities\n",
    "    for en in ens:\n",
    "\n",
    "        entype = en[1]\n",
    "        type_id = entity_id[entype]\n",
    "        \n",
    "        s_idx, e_idx = mappings[en[-1]]\n",
    "        word_info1 = nnsents[s_idx]\n",
    "        word_info2 = nnsents[e_idx]\n",
    "        \n",
    "        sent_idx1 = word_info1[3][0]\n",
    "        sent_idx2 = word_info2[3][0]\n",
    "        sent_with_entities.add(sent_idx1)\n",
    "    #         sent_with_entities.add(sent_idx2)\n",
    "        \n",
    "        try:\n",
    "            sent_text = get_sent(nsents, sent_idx1, sent_idx2)\n",
    "            context = \" \".join([e[0] for e in sent_text])\n",
    "            start = word_info1[3][1]\n",
    "            end = word_info2[3][1]\n",
    "            start_end = f\"{start};{end}\"\n",
    "        except Exception as ex:\n",
    "            print(fn.name)\n",
    "            print(word_info1, word_info2)\n",
    "            print(context)\n",
    "            print(start, end, start_end)\n",
    "            print(entype, type_id)\n",
    "        \n",
    "        # key will be sent_id, type\n",
    "        # data will be tuple (context, entype, type_id, start, end, start_end)\n",
    "        entity_sent_idx_mappings[(sent_idx1, entype)].append(\n",
    "            (context, entype, type_id, start, end, start_end))\n",
    "\n",
    "\n",
    "    # for i in sent_ids:\n",
    "    for i in sent_with_entities:\n",
    "        file_id = f\"{fn.stem}\"\n",
    "        sent_id = f\"{i}\"\n",
    "        sent_i_context = \" \".join(e[0] for e in nsents[i])\n",
    "        for k, v in entity_query_template.items():\n",
    "            tid = entity_id[k]\n",
    "            \n",
    "            if (i, k) in entity_sent_idx_mappings:\n",
    "                d = {\n",
    "                        \"context\": sent_i_context,\n",
    "                        \"end_position\": [],\n",
    "                        \"entity_label\": k,\n",
    "                        \"impossible\": False,\n",
    "                        \"qas_id\": f\"{file_id}.{sent_id}.{sent_id}.{tid}\",\n",
    "                        \"query\": v,\n",
    "                        \"span_position\": [],\n",
    "                        \"start_position\": []\n",
    "                        }\n",
    "                \n",
    "                entities = entity_sent_idx_mappings[(i, k)]\n",
    "        \n",
    "                for ent in entities:\n",
    "                    assert ent[0] == sent_i_context, f\"expect context: {ent[0]} but get {sent_i_context}\"\n",
    "                    assert ent[1] == k, f\"expect en type: {ent[1]} but get {k}\"\n",
    "                    s, e, se = ent[-3:]\n",
    "                    d[\"start_position\"].append(s)\n",
    "                    d[\"end_position\"].append(e)\n",
    "                    d[\"span_position\"].append(se)\n",
    "                \n",
    "                training_data.append(d)\n",
    "            else:\n",
    "                d = {\n",
    "                        \"context\": sent_i_context,\n",
    "                        \"end_position\": [],\n",
    "                        \"entity_label\": k,\n",
    "                        \"impossible\": True,\n",
    "                        \"qas_id\": f\"{file_id}.{sent_id}.{sent_id}.{tid}\",\n",
    "                        \"query\": v,\n",
    "                        \"span_position\": [],\n",
    "                        \"start_position\": []\n",
    "                    }\n",
    "                training_data.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pout = Path(\"/data/datasets/cheng/mrc-for-ner-medical/2018_n2c2/data/mrc_entity_new\")\n",
    "# pout = Path(\"/data/datasets/cheng/mrc-for-ner-medical/2018_n2c2/data/mrc_trigger\")\n",
    "pout.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_json(training_data, pout, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##single sentence as a sample\n",
    "\n",
    "dev_data = []\n",
    "\n",
    "\n",
    "for i, fn in enumerate(p_dev.glob(\"*.ann\")):\n",
    "\n",
    "    txt_fn = p_dev / f\"{fn.stem}.txt\"\n",
    "    ann_fn = p_dev / f\"{fn.name}\"\n",
    "    txt, sents = pre_processing(txt_fn, MIMICIII_PATTERN, max_len=256)\n",
    "    e2i, ens, _, evns, attrs = load_annotation_brat(ann_fn)\n",
    "    i2e = {v: k for k, v in e2i.items()}\n",
    "    nsents, sent_bound = generate_BIO(sents, ens, file_id=\"\", no_overlap=False, record_pos=True)\n",
    "    nnsents = [w for sent in nsents for w in sent]\n",
    "    mappings = create_entity_to_sent_mapping(nnsents, ens, i2e, fn.name)\n",
    "\n",
    "    num_sents = len(nsents)\n",
    "    sent_ids = set(range(num_sents))\n",
    "    sent_with_entities = set()\n",
    "\n",
    "    entity_sent_idx_mappings = defaultdict(list)\n",
    "\n",
    "    # sentence with entities\n",
    "    for en in ens:\n",
    "\n",
    "        entype = en[1]\n",
    "        type_id = entity_id[entype]\n",
    "        \n",
    "        s_idx, e_idx = mappings[en[-1]]\n",
    "        word_info1 = nnsents[s_idx]\n",
    "        word_info2 = nnsents[e_idx]\n",
    "        \n",
    "        sent_idx1 = word_info1[3][0]\n",
    "        sent_idx2 = word_info2[3][0]\n",
    "        sent_with_entities.add(sent_idx1)\n",
    "    #         sent_with_entities.add(sent_idx2)\n",
    "        \n",
    "        try:\n",
    "            sent_text = get_sent(nsents, sent_idx1, sent_idx2)\n",
    "            context = \" \".join([e[0] for e in sent_text])\n",
    "            start = word_info1[3][1]\n",
    "            end = word_info2[3][1]\n",
    "            start_end = f\"{start};{end}\"\n",
    "        except Exception as ex:\n",
    "            print(fn.name)\n",
    "            print(word_info1, word_info2)\n",
    "            print(context)\n",
    "            print(start, end, start_end)\n",
    "            print(entype, type_id)\n",
    "        \n",
    "        # key will be sent_id, type\n",
    "        # data will be tuple (context, entype, type_id, start, end, start_end)\n",
    "        entity_sent_idx_mappings[(sent_idx1, entype)].append(\n",
    "            (context, entype, type_id, start, end, start_end))\n",
    "\n",
    "    # for i in sent_ids:\n",
    "    for i in sent_with_entities:\n",
    "        file_id = f\"{fn.stem}\"\n",
    "        sent_id = f\"{i}\"\n",
    "        sent_i_context = \" \".join(e[0] for e in nsents[i])\n",
    "        for k, v in entity_query_template.items():\n",
    "            tid = entity_id[k]\n",
    "            if (i, k) in entity_sent_idx_mappings:\n",
    "                d = {\n",
    "                        \"context\": sent_i_context,\n",
    "                        \"end_position\": [],\n",
    "                        \"entity_label\": k,\n",
    "                        \"impossible\": False,\n",
    "                        \"qas_id\": f\"{file_id}.{sent_id}.{sent_id}.{tid}\",\n",
    "                        \"query\": v,\n",
    "                        \"span_position\": [],\n",
    "                        \"start_position\": []\n",
    "                        }\n",
    "                \n",
    "                entities = entity_sent_idx_mappings[(i, k)]\n",
    "        \n",
    "                for ent in entities:\n",
    "                    assert ent[0] == sent_i_context, f\"expect context: {ent[0]} but get {sent_i_context}\"\n",
    "                    assert ent[1] == k, f\"expect en type: {ent[1]} but get {k}\"\n",
    "                    s, e, se = ent[-3:]\n",
    "                    d[\"start_position\"].append(s)\n",
    "                    d[\"end_position\"].append(e)\n",
    "                    d[\"span_position\"].append(se)\n",
    "                \n",
    "                dev_data.append(d)\n",
    "            else:\n",
    "                d = {\n",
    "                        \"context\": sent_i_context,\n",
    "                        \"end_position\": [],\n",
    "                        \"entity_label\": k,\n",
    "                        \"impossible\": True,\n",
    "                        \"qas_id\": f\"{file_id}.{sent_id}.{sent_id}.{tid}\",\n",
    "                        \"query\": v,\n",
    "                        \"span_position\": [],\n",
    "                        \"start_position\": []\n",
    "                    }\n",
    "                dev_data.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_json(dev_data, pout, \"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##single sentence as a sample\n",
    "##test dataset\n",
    "test_data = []\n",
    "\n",
    "\n",
    "for i, fn in enumerate(p_dev.glob(\"*.ann\")):\n",
    "\n",
    "    txt_fn = p_dev / f\"{fn.stem}.txt\"\n",
    "    ann_fn = p_dev / f\"{fn.name}\"\n",
    "    txt, sents = pre_processing(txt_fn, MIMICIII_PATTERN, max_len=256)\n",
    "    e2i, ens, _, evns, attrs = load_annotation_brat(ann_fn)\n",
    "    i2e = {v: k for k, v in e2i.items()}\n",
    "    nsents, sent_bound = generate_BIO(sents, ens, file_id=\"\", no_overlap=False, record_pos=True)\n",
    "    nnsents = [w for sent in nsents for w in sent]\n",
    "    mappings = create_entity_to_sent_mapping(nnsents, ens, i2e, fn.name)\n",
    "\n",
    "    num_sents = len(nsents)\n",
    "    sent_ids = set(range(num_sents))\n",
    "    sent_with_entities = set()\n",
    "\n",
    "    entity_sent_idx_mappings = defaultdict(list)\n",
    "\n",
    "    # sentence with entities\n",
    "    for en in ens:\n",
    "\n",
    "        entype = en[1]\n",
    "        type_id = entity_id[entype]\n",
    "        \n",
    "        s_idx, e_idx = mappings[en[-1]]\n",
    "        word_info1 = nnsents[s_idx]\n",
    "        word_info2 = nnsents[e_idx]\n",
    "        \n",
    "        sent_idx1 = word_info1[3][0]\n",
    "        sent_idx2 = word_info2[3][0]\n",
    "        sent_with_entities.add(sent_idx1)\n",
    "    #         sent_with_entities.add(sent_idx2)\n",
    "\n",
    "    # for i in sent_ids:\n",
    "    for i in sent_with_entities:\n",
    "        file_sent_id = f\"{fn.stem}_{i}\"\n",
    "        sent_i_context = \" \".join(e[0] for e in nsents[i])\n",
    "        for k, v in entity_query_template.items():\n",
    "            tid = entity_id[k]\n",
    "            d = {\n",
    "                    \"context\": sent_i_context,\n",
    "                    \"end_position\": [],\n",
    "                    \"entity_label\": k,\n",
    "                    \"impossible\": True,\n",
    "                    \"qas_id\": f\"{file_sent_id}.{tid}\",\n",
    "                    \"query\": v,\n",
    "                    \"span_position\": [],\n",
    "                    \"start_position\": []\n",
    "                    }\n",
    "            test_data.append(d)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_json(test_data, pout, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##single sentence as a sample\n",
    "# relation extraction\n",
    "\n",
    "training_data = []\n",
    "\n",
    "\n",
    "for i, fn in enumerate(p_tr.glob(\"*.ann\")):\n",
    "\n",
    "    txt_fn = p_tr / f\"{fn.stem}.txt\"\n",
    "    ann_fn = p_tr / f\"{fn.name}\"\n",
    "    txt, sents = pre_processing(txt_fn, MIMICIII_PATTERN, max_len=256)\n",
    "    e2i, ens, relations, evns, attrs = load_annotation_brat(ann_fn)\n",
    "    i2e = {v: k for k, v in e2i.items()}\n",
    "    nsents, sent_bound = generate_BIO(sents, ens, file_id=\"\", no_overlap=False, record_pos=True)\n",
    "    nnsents = [w for sent in nsents for w in sent]\n",
    "    mappings = create_entity_to_sent_mapping(nnsents, ens, i2e, fn.name)\n",
    "\n",
    "    num_sents = len(nsents)\n",
    "    sent_ids = set(range(num_sents))\n",
    "    sent_with_drugs = set()\n",
    "\n",
    "    drug_lists = []\n",
    "    attributes_lists = []\n",
    "    tail_ann_idx_mappings = defaultdict(list)\n",
    "    relation_idx_mappings = defaultdict(list)\n",
    "\n",
    "\n",
    "    # sentence with entities\n",
    "    for en in ens:\n",
    "        entype = en[1]\n",
    "        type_id = entity_id[entype]\n",
    "        text = en[0]\n",
    "        s_idx, e_idx = mappings[en[-1]]\n",
    "        word_info1 = nnsents[s_idx]\n",
    "        word_info2 = nnsents[e_idx]\n",
    "        \n",
    "        sent_idx1 = word_info1[3][0]\n",
    "        sent_idx2 = word_info2[3][0]\n",
    "        sent_with_drugs.add(sent_idx1)\n",
    "        \n",
    "        try:\n",
    "            sent_text = get_sent(nsents, sent_idx1, sent_idx2)\n",
    "            context = \" \".join([e[0] for e in sent_text])\n",
    "            start = word_info1[3][1]\n",
    "            end = word_info2[3][1]\n",
    "            start_end = f\"{start};{end}\"\n",
    "        except Exception as ex:\n",
    "            print(fn.name)\n",
    "            print(word_info1, word_info2)\n",
    "            print(context)\n",
    "            print(start, end, start_end)\n",
    "            print(entype, type_id)\n",
    "            \n",
    "            # key will be sent_id, type\n",
    "            # data will be tuple (context, entype, type_id, start, end, start_end)\n",
    "        if entype == \"Drug\":\n",
    "            drug_lists.append(\n",
    "                [sent_idx1, text, context, start, end, start_end, en[-1]])\n",
    "        else:\n",
    "            tail_ann_idx_mappings[(en[-1])].append((sent_idx1, text, context, type_id, start, end, start_end))\n",
    "\n",
    "    for pair in relations:\n",
    "        relation_idx_mappings[(pair[-1].split(\":\")[1],pair[0].split('-')[0])].append((pair[1].split(\":\")[1]))\n",
    "       \n",
    "\n",
    "    for drug_ent in drug_lists:\n",
    "        sent_id = drug_ent[0]\n",
    "        drug_text = drug_ent[1]\n",
    "        len_sent = len(nsents[sent_id])\n",
    "        file_sent_id = f\"{fn.stem}.{sent_id}\"\n",
    "        drug_sent_context = \" \".join(e[0] for e in nsents[drug_ent[0]])\n",
    "        ann_id = drug_ent[-1]\n",
    "        for k, v in attribute_query_template.items():\n",
    "            tid = attribute_id[k]\n",
    "            \n",
    "            if (ann_id, k) in relation_idx_mappings:\n",
    "                \n",
    "                attributes = relation_idx_mappings[(ann_id, k)]\n",
    "\n",
    "                sstart=[]\n",
    "                eend=[]\n",
    "                sspan=[]\n",
    "        \n",
    "                for ent in attributes:\n",
    "                    tail_ann_id = ent\n",
    "                    tail_sent_id = tail_ann_idx_mappings[(tail_ann_id)][0][0]\n",
    "                    tail_context = tail_ann_idx_mappings[(tail_ann_id)][0][2]\n",
    "                    if sent_id == tail_sent_id:\n",
    "                        s, e, se = tail_ann_idx_mappings[(tail_ann_id)][0][-3:]\n",
    "                        sstart.append(s)\n",
    "                        eend.append(e)\n",
    "                        sspan.append(se)\n",
    "                        d = {\n",
    "                                \"context\": drug_sent_context,\n",
    "                                \"end_position\": eend,\n",
    "                                \"entity_label\": k,\n",
    "                                \"impossible\": False,\n",
    "                                \"qas_id\": f\"{file_sent_id}.{tail_sent_id}.{tid}\",\n",
    "                                \"query\": v.format(drug_text),\n",
    "                                \"span_position\": sspan,\n",
    "                                \"start_position\": sstart\n",
    "                            }\n",
    "\n",
    "                    if sent_id < tail_sent_id:\n",
    "                        s= tail_ann_idx_mappings[(tail_ann_id)][0][-3] + len_sent\n",
    "                        e= tail_ann_idx_mappings[(tail_ann_id)][0][-2] + len_sent\n",
    "                        se= f\"{s};{e}\"\n",
    "                        dd = {\n",
    "                                \"context\": drug_sent_context + ' ' + tail_context,\n",
    "                                \"end_position\": [e],\n",
    "                                \"entity_label\": k,\n",
    "                                \"impossible\": False,\n",
    "                                \"qas_id\": f\"{file_sent_id}.{tail_sent_id}.{tid}\",\n",
    "                                \"query\": v.format(drug_text),\n",
    "                                \"span_position\": [se],\n",
    "                                \"start_position\": [s]\n",
    "                            }\n",
    "                        training_data.append(dd)\n",
    "                    if sent_id > tail_sent_id:\n",
    "                        s= tail_ann_idx_mappings[(tail_ann_id)][0][-3] \n",
    "                        e= tail_ann_idx_mappings[(tail_ann_id)][0][-2]\n",
    "                        se= f\"{s};{e}\"\n",
    "                        dd = {\n",
    "                                \"context\": tail_context + ' ' + drug_sent_context,\n",
    "                                \"end_position\": [e],\n",
    "                                \"entity_label\": k,\n",
    "                                \"impossible\": False,\n",
    "                                \"qas_id\": f\"{file_sent_id}.{tail_sent_id}.{tid}\",\n",
    "                                \"query\": v.format(drug_text),\n",
    "                                \"span_position\": [se],\n",
    "                                \"start_position\": [s]\n",
    "                            }\n",
    "                        training_data.append(dd)\n",
    "                training_data.append(d)\n",
    "                 \n",
    "            else:\n",
    "                d = {\n",
    "                        \"context\": drug_sent_context,\n",
    "                        \"end_position\": [],\n",
    "                        \"entity_label\": k,\n",
    "                        \"impossible\": True,\n",
    "                        \"qas_id\": f\"{file_sent_id}.{sent_id}.{tid}\",\n",
    "                        \"query\": v.format(drug_text),\n",
    "                        \"span_position\": [],\n",
    "                        \"start_position\": []\n",
    "                    }\n",
    "                training_data.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pout = Path(\"/data/datasets/cheng/mrc-for-ner-medical/2018_n2c2/data/mrc_relation\")\n",
    "# pout = Path(\"/data/datasets/cheng/mrc-for-ner-medical/2018_n2c2/data/mrc_trigger\")\n",
    "pout.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_json(training_data, pout, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##single sentence as a dev sample\n",
    "# relation extraction\n",
    "\n",
    "dev_data = []\n",
    "\n",
    "\n",
    "for i, fn in enumerate(p_dev.glob(\"*.ann\")):\n",
    "\n",
    "    txt_fn = p_dev / f\"{fn.stem}.txt\"\n",
    "    ann_fn = p_dev / f\"{fn.name}\"\n",
    "    txt, sents = pre_processing(txt_fn, MIMICIII_PATTERN, max_len=256)\n",
    "    e2i, ens, relations, evns, attrs = load_annotation_brat(ann_fn)\n",
    "    i2e = {v: k for k, v in e2i.items()}\n",
    "    nsents, sent_bound = generate_BIO(sents, ens, file_id=\"\", no_overlap=False, record_pos=True)\n",
    "    nnsents = [w for sent in nsents for w in sent]\n",
    "    mappings = create_entity_to_sent_mapping(nnsents, ens, i2e, fn.name)\n",
    "\n",
    "    num_sents = len(nsents)\n",
    "    sent_ids = set(range(num_sents))\n",
    "    sent_with_drugs = set()\n",
    "\n",
    "    drug_lists = []\n",
    "    attributes_lists = []\n",
    "    tail_ann_idx_mappings = defaultdict(list)\n",
    "    relation_idx_mappings = defaultdict(list)\n",
    "\n",
    "\n",
    "    # sentence with entities\n",
    "    for en in ens:\n",
    "        entype = en[1]\n",
    "        type_id = entity_id[entype]\n",
    "        text = en[0]\n",
    "        s_idx, e_idx = mappings[en[-1]]\n",
    "        word_info1 = nnsents[s_idx]\n",
    "        word_info2 = nnsents[e_idx]\n",
    "        \n",
    "        sent_idx1 = word_info1[3][0]\n",
    "        sent_idx2 = word_info2[3][0]\n",
    "        sent_with_drugs.add(sent_idx1)\n",
    "        \n",
    "        try:\n",
    "            sent_text = get_sent(nsents, sent_idx1, sent_idx2)\n",
    "            context = \" \".join([e[0] for e in sent_text])\n",
    "            start = word_info1[3][1]\n",
    "            end = word_info2[3][1]\n",
    "            start_end = f\"{start};{end}\"\n",
    "        except Exception as ex:\n",
    "            print(fn.name)\n",
    "            print(word_info1, word_info2)\n",
    "            print(context)\n",
    "            print(start, end, start_end)\n",
    "            print(entype, type_id)\n",
    "            \n",
    "            # key will be sent_id, type\n",
    "            # data will be tuple (context, entype, type_id, start, end, start_end)\n",
    "        if entype == \"Drug\":\n",
    "            drug_lists.append(\n",
    "                [sent_idx1, text, context, start, end, start_end, en[-1]])\n",
    "        else:\n",
    "            tail_ann_idx_mappings[(en[-1])].append((sent_idx1, text, context, type_id, start, end, start_end))\n",
    "\n",
    "    for pair in relations:\n",
    "        relation_idx_mappings[(pair[-1].split(\":\")[1],pair[0].split('-')[0])].append((pair[1].split(\":\")[1]))\n",
    "       \n",
    "        \n",
    "\n",
    "\n",
    "    # for i in sent_ids:\n",
    "    # for i in sent_with_drugs:\n",
    "    #     file_sent_id = f\"{fn.stem}_{i}\"\n",
    "    #     sent_i_context = \" \".join(e[0] for e in nsents[i])\n",
    "    #     drug_entities = entity_sent_idx_mappings[(i, \"Drug\")]\n",
    "\n",
    "    for drug_ent in drug_lists:\n",
    "        sent_id = drug_ent[0]\n",
    "        drug_text = drug_ent[1]\n",
    "        len_sent = len(nsents[sent_id])\n",
    "        file_sent_id = f\"{fn.stem}.{sent_id}\"\n",
    "        drug_sent_context = \" \".join(e[0] for e in nsents[drug_ent[0]])\n",
    "        ann_id = drug_ent[-1]\n",
    "        for k, v in attribute_query_template.items():\n",
    "            tid = attribute_id[k]\n",
    "            \n",
    "            if (ann_id, k) in relation_idx_mappings:\n",
    "                \n",
    "                attributes = relation_idx_mappings[(ann_id, k)]\n",
    "\n",
    "                sstart=[]\n",
    "                eend=[]\n",
    "                sspan=[]\n",
    "        \n",
    "                for ent in attributes:\n",
    "                    tail_ann_id = ent\n",
    "                    tail_sent_id = tail_ann_idx_mappings[(tail_ann_id)][0][0]\n",
    "                    tail_context = tail_ann_idx_mappings[(tail_ann_id)][0][2]\n",
    "                    if sent_id == tail_sent_id:\n",
    "                        s, e, se = tail_ann_idx_mappings[(tail_ann_id)][0][-3:]\n",
    "                        sstart.append(s)\n",
    "                        eend.append(e)\n",
    "                        sspan.append(se)\n",
    "                        d = {\n",
    "                                \"context\": drug_sent_context,\n",
    "                                \"end_position\": eend,\n",
    "                                \"entity_label\": k,\n",
    "                                \"impossible\": False,\n",
    "                                \"qas_id\": f\"{file_sent_id}.{tail_sent_id}.{tid}\",\n",
    "                                \"query\": v.format(drug_text),\n",
    "                                \"span_position\": sspan,\n",
    "                                \"start_position\": sstart\n",
    "                            }\n",
    "\n",
    "                    if sent_id < tail_sent_id:\n",
    "                        s= tail_ann_idx_mappings[(tail_ann_id)][0][-3] + len_sent\n",
    "                        e= tail_ann_idx_mappings[(tail_ann_id)][0][-2] + len_sent\n",
    "                        se= f\"{s};{e}\"\n",
    "                        dd = {\n",
    "                                \"context\": drug_sent_context + ' ' + tail_context,\n",
    "                                \"end_position\": [e],\n",
    "                                \"entity_label\": k,\n",
    "                                \"impossible\": False,\n",
    "                                \"qas_id\": f\"{file_sent_id}.{tail_sent_id}.{tid}\",\n",
    "                                \"query\": v.format(drug_text),\n",
    "                                \"span_position\": [se],\n",
    "                                \"start_position\": [s]\n",
    "                            }\n",
    "                        dev_data.append(dd)\n",
    "                    if sent_id > tail_sent_id:\n",
    "                        s= tail_ann_idx_mappings[(tail_ann_id)][0][-3] \n",
    "                        e= tail_ann_idx_mappings[(tail_ann_id)][0][-2]\n",
    "                        se= f\"{s};{e}\"\n",
    "                        dd = {\n",
    "                                \"context\": tail_context + ' ' + drug_sent_context,\n",
    "                                \"end_position\": [e],\n",
    "                                \"entity_label\": k,\n",
    "                                \"impossible\": False,\n",
    "                                \"qas_id\": f\"{file_sent_id}.{tail_sent_id}.{tid}\",\n",
    "                                \"query\": v.format(drug_text),\n",
    "                                \"span_position\": [se],\n",
    "                                \"start_position\": [s]\n",
    "                            }\n",
    "                        dev_data.append(dd)\n",
    "                dev_data.append(d)\n",
    "                 \n",
    "            else:\n",
    "                d = {\n",
    "                        \"context\": drug_sent_context,\n",
    "                        \"end_position\": [],\n",
    "                        \"entity_label\": k,\n",
    "                        \"impossible\": True,\n",
    "                        \"qas_id\": f\"{file_sent_id}.{sent_id}.{tid}\",\n",
    "                        \"query\": v.format(drug_text),\n",
    "                        \"span_position\": [],\n",
    "                        \"start_position\": []\n",
    "                    }\n",
    "                dev_data.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_json(dev_data, pout, \"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##single sentence as a test sample\n",
    "# relation extraction\n",
    "\n",
    "test_data = []\n",
    "\n",
    "p_pred = Path(\"/data/datasets/cheng/mrc-for-ner-medical/2018_n2c2/exp/ner/results/test_1\")\n",
    "# for i, fn in enumerate(p_dev.glob(\"*.ann\")):\n",
    "for i, fn in enumerate(p_dev.glob(\"*.txt\")):\n",
    "\n",
    "    txt_fn = p_dev / f\"{fn.name}\"\n",
    "    # ann_fn = p_dev / f\"{fn.stem}.ann\"\n",
    "    ann_fn = p_pred / f\"{fn.stem}.ann\"\n",
    "    txt, sents = pre_processing(txt_fn, MIMICIII_PATTERN, max_len=256)\n",
    "    e2i, ens, relations, evns, attrs = load_annotation_brat(ann_fn)\n",
    "    i2e = {v: k for k, v in e2i.items()}\n",
    "    nsents, sent_bound = generate_BIO(sents, ens, file_id=\"\", no_overlap=False, record_pos=True)\n",
    "    nnsents = [w for sent in nsents for w in sent]\n",
    "    mappings = create_entity_to_sent_mapping(nnsents, ens, i2e, fn.name)\n",
    "\n",
    "    num_sents = len(nsents)\n",
    "    sent_ids = set(range(num_sents))\n",
    "    sent_with_drugs = set()\n",
    "\n",
    "    drug_lists = []\n",
    "    attributes_lists = []\n",
    "    tail_ann_idx_mappings = defaultdict(list)\n",
    "    relation_idx_mappings = defaultdict(list)\n",
    "\n",
    "\n",
    "    # sentence with entities\n",
    "    for en in ens:\n",
    "        entype = en[1]\n",
    "        type_id = entity_id[entype]\n",
    "        text = en[0]\n",
    "        s_idx, e_idx = mappings[en[-1]]\n",
    "        word_info1 = nnsents[s_idx]\n",
    "        word_info2 = nnsents[e_idx]\n",
    "        \n",
    "        sent_idx1 = word_info1[3][0]\n",
    "        sent_idx2 = word_info2[3][0]\n",
    "        sent_with_drugs.add(sent_idx1)\n",
    "        \n",
    "        try:\n",
    "            sent_text = get_sent(nsents, sent_idx1, sent_idx2)\n",
    "            context = \" \".join([e[0] for e in sent_text])\n",
    "            start = word_info1[3][1]\n",
    "            end = word_info2[3][1]\n",
    "            start_end = f\"{start};{end}\"\n",
    "        except Exception as ex:\n",
    "            print(fn.name)\n",
    "            print(word_info1, word_info2)\n",
    "            print(context)\n",
    "            print(start, end, start_end)\n",
    "            print(entype, type_id)\n",
    "            \n",
    "            # key will be sent_id, type\n",
    "            # data will be tuple (context, entype, type_id, start, end, start_end)\n",
    "        if entype == \"Drug\":\n",
    "            # drug_lists.append([sent_idx1, text, context, start, end, start_end, en[-1]])\n",
    "            drug_lists.append([sent_idx1, text, context, start, end, start_end, en[-2], en[-1]])\n",
    "\n",
    "        else:\n",
    "            tail_ann_idx_mappings[(en[-1])].append((sent_idx1, text, context, type_id, start, end, start_end))\n",
    "\n",
    "    for pair in relations:\n",
    "        relation_idx_mappings[(pair[-1].split(\":\")[1],pair[0].split('-')[0])].append((pair[1].split(\":\")[1]))\n",
    "       \n",
    "        \n",
    "\n",
    "\n",
    "    # for i in sent_ids:\n",
    "    # for i in sent_with_drugs:\n",
    "    #     file_sent_id = f\"{fn.stem}_{i}\"\n",
    "    #     sent_i_context = \" \".join(e[0] for e in nsents[i])\n",
    "    #     drug_entities = entity_sent_idx_mappings[(i, \"Drug\")]\n",
    "\n",
    "    for drug_ent in drug_lists:\n",
    "        sent_id = drug_ent[0]\n",
    "        drug_text = drug_ent[1]\n",
    "        drug_ann = drug_ent[-1]\n",
    "        len_sent = len(nsents[sent_id])\n",
    "        file_sent_id = f\"{fn.stem}.{sent_id}\"\n",
    "        drug_sent_context = \" \".join(e[0] for e in nsents[drug_ent[0]])\n",
    "        ann_id = drug_ent[-1]\n",
    "        for k, v in attribute_query_template.items():\n",
    "            tid = attribute_id[k]\n",
    "            \n",
    "            if (ann_id, k) in relation_idx_mappings:\n",
    "                \n",
    "                attributes = relation_idx_mappings[(ann_id, k)]\n",
    "\n",
    "                sstart=[]\n",
    "                eend=[]\n",
    "                sspan=[]\n",
    "        \n",
    "                for ent in attributes:\n",
    "                    tail_ann_id = ent\n",
    "                    tail_sent_id = tail_ann_idx_mappings[(tail_ann_id)][0][0]\n",
    "                    tail_context = tail_ann_idx_mappings[(tail_ann_id)][0][2]\n",
    "                    if sent_id == tail_sent_id:\n",
    "                        s, e, se = tail_ann_idx_mappings[(tail_ann_id)][0][-3:]\n",
    "                        sstart.append(s)\n",
    "                        eend.append(e)\n",
    "                        sspan.append(se)\n",
    "                        d = {\n",
    "                                \"context\": drug_sent_context,\n",
    "                                \"end_position\": eend,\n",
    "                                \"entity_label\": [k, drug_text, drug_ent[-2][0],drug_ent[-2][1]],\n",
    "                                \"impossible\": False,\n",
    "                                \"qas_id\": f\"{file_sent_id}.{tail_sent_id}.{tid}\",\n",
    "                                \"query\": v.format(drug_text),\n",
    "                                \"span_position\": sspan,\n",
    "                                \"start_position\": sstart\n",
    "                            }\n",
    "\n",
    "                    if sent_id < tail_sent_id:\n",
    "                        s= tail_ann_idx_mappings[(tail_ann_id)][0][-3] + len_sent\n",
    "                        e= tail_ann_idx_mappings[(tail_ann_id)][0][-2] + len_sent\n",
    "                        se= f\"{s};{e}\"\n",
    "                        dd = {\n",
    "                                \"context\": drug_sent_context + ' ' + tail_context,\n",
    "                                \"end_position\": [e],\n",
    "                                \"entity_label\": [k, drug_text, drug_ent[-2][0],drug_ent[-2][1]],\n",
    "                                \"impossible\": False,\n",
    "                                \"qas_id\": f\"{file_sent_id}.{tail_sent_id}.{tid}\",\n",
    "                                \"query\": v.format(drug_text),\n",
    "                                \"span_position\": [se],\n",
    "                                \"start_position\": [s]\n",
    "                            }\n",
    "                        test_data.append(dd)\n",
    "                    if sent_id > tail_sent_id:\n",
    "                        s= tail_ann_idx_mappings[(tail_ann_id)][0][-3] \n",
    "                        e= tail_ann_idx_mappings[(tail_ann_id)][0][-2]\n",
    "                        se= f\"{s};{e}\"\n",
    "                        dd = {\n",
    "                                \"context\": tail_context + ' ' + drug_sent_context,\n",
    "                                \"end_position\": [e],\n",
    "                                \"entity_label\": [k, drug_text, drug_ent[-2][0],drug_ent[-2][1]],\n",
    "                                \"impossible\": False,\n",
    "                                \"qas_id\": f\"{file_sent_id}.{tail_sent_id}.{tid}\",\n",
    "                                \"query\": v.format(drug_text),\n",
    "                                \"span_position\": [se],\n",
    "                                \"start_position\": [s]\n",
    "                            }\n",
    "                        test_data.append(dd)\n",
    "                test_data.append(d)\n",
    "                 \n",
    "            else:\n",
    "                d = {\n",
    "                        \"context\": drug_sent_context,\n",
    "                        \"end_position\": [],\n",
    "                        \"entity_label\": [k, drug_text, drug_ent[-2][0],drug_ent[-2][1]],\n",
    "                        \"impossible\": True,\n",
    "                        \"qas_id\": f\"{file_sent_id}.{sent_id}.{tid}\",\n",
    "                        \"query\": v.format(drug_text),\n",
    "                        \"span_position\": [],\n",
    "                        \"start_position\": []\n",
    "                    }\n",
    "                test_data.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_json(test_data, pout, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_index_to_wordindex (token_start, token_end, tokens, ccontext, nnsents, sent_id):\n",
    "\n",
    "    corpora_records = ccontext.split()\n",
    "    word_2_char_mapping={}\n",
    "    char_cursor=0\n",
    "    for ind in range(len(corpora_records)):\n",
    "        if(len(corpora_records[ind])>0):#the last space will not be considered\n",
    "            start=char_cursor\n",
    "            end=char_cursor+len(corpora_records[ind])\n",
    "            word_2_char_mapping[ind]=[start,end]\n",
    "            char_cursor=char_cursor+len(corpora_records[ind])+1#consider the white-space length\n",
    "    # print(ccontext)\n",
    "    # print(word_2_char_mapping)\n",
    "    start_char_span=tokens.token_to_chars(token_start)\n",
    "    end_char_span=tokens.token_to_chars(token_end)\n",
    "    # print(start_char_span,end_char_span)\n",
    "    for each_word in word_2_char_mapping:\n",
    "        start = word_2_char_mapping[each_word][0]\n",
    "        end = word_2_char_mapping[each_word][1]\n",
    "        \n",
    "        if(start_char_span[0]>=start and start_char_span[1]<=end):\n",
    "            print('a')\n",
    "            s_char = each_word\n",
    "\n",
    "        if(end_char_span[0]>=start and end_char_span[1]<=end):\n",
    "            print('b')\n",
    "            e_char = each_word\n",
    "    # print(sent_id, s_char, e_char, ccontext)\n",
    "    for i, sent in enumerate(nnsents):\n",
    "        if (sent_id, s_char) == sent[3]:\n",
    "            print('c')\n",
    "            s_offset = sent[1][0]\n",
    "            # print(s_offset)\n",
    "            # word_idx_s = i \n",
    "        if (sent_id, e_char) == sent[3]:\n",
    "            print('d')\n",
    "            e_offset = sent[1][1]\n",
    "            # print(e_offset)\n",
    "            # word_idx_e = i\n",
    "    return s_offset, e_offset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the output to brat format\n",
    "#NER task\n",
    "BRAT_TEMPLATE_T = \"{}\\t{} {} {}\\t{}\"\n",
    "output_template_t = BRAT_TEMPLATE_T\n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "p_test = Path(\"/data/datasets/cheng/mrc-for-ner-medical/2018_n2c2/test_data/gold_standard_test\")\n",
    "\n",
    "mrc_entity_dir = Path('/data/datasets/cheng/mrc-for-ner-medical/2018_n2c2/data/mrc_entity/mrc-ner.dev')\n",
    "\n",
    "entity_pred_dir = Path('/data/datasets/cheng/mrc-for-ner-medical/2018_n2c2/exp/ner/pred')\n",
    "\n",
    "entity_model = 'pred_1203_bert-large-cased_2_4_3e-5_20'\n",
    "\n",
    "entity_pred_fn = entity_pred_dir / f\"{entity_model}.json\"\n",
    "\n",
    "p_output = Path('/data/datasets/cheng/mrc-for-ner-medical/2018_n2c2/exp/ner/results/test_1')\n",
    "\n",
    "bert_path = \"/home/alexgre/projects/transformer_pretrained_models/bert-large-cased\"\n",
    "vocab_file = os.path.join(bert_path, \"vocab.txt\")\n",
    "tokenizer = BertWordPieceTokenizer(vocab_file)\n",
    "\n",
    "with open(entity_pred_fn, \"r\") as f:\n",
    "    entity_preds = json.load(f)\n",
    "\n",
    "with open(mrc_entity_dir, \"r\") as f:\n",
    "    mrc_entity_fn = json.load(f)\n",
    "\n",
    "file_suffix = 'ann'\n",
    "\n",
    "for i, fn in enumerate(p_test.glob(\"*.txt\")):\n",
    "    \n",
    "    txt_fn = p_test / f\"{fn.name}\"\n",
    "    txt_contents = open(txt_fn,'r').read()\n",
    "    \n",
    "    txt, sents = pre_processing(txt_fn, MIMICIII_PATTERN, max_len=256)\n",
    "    output_fn = p_output / \"{}.{}\".format(fn.stem, file_suffix)\n",
    "    \n",
    "    nsents, sent_bound = generate_BIO(sents, [], file_id=\"\", no_overlap=False, record_pos=True)\n",
    "    nnsents = [w for sent in nsents for w in sent]\n",
    "\n",
    "    num_sents = len(nsents)\n",
    "    sent_ids = set(range(num_sents))\n",
    "\n",
    "    entity_pred = []\n",
    "    entities_T = []\n",
    "    print(fn.stem)\n",
    "    # print(nnsents)\n",
    "    \n",
    "  \n",
    "    \n",
    "    for i, pred in enumerate(entity_preds):\n",
    "        sample_idx = str(pred['sample_idx'][0][0])[:6]\n",
    "        sample_idx = int(sample_idx)\n",
    "        if sample_idx == int(fn.stem):\n",
    "            entity_pred.append([i,pred])\n",
    "    \n",
    "    for pred_e in entity_pred:\n",
    "        idx = pred_e[0]\n",
    "        ent_r = pred_e[1]\n",
    "        sample_idx = str(ent_r['sample_idx'][0][0])[6:]\n",
    "        sent_id = int(sample_idx)\n",
    "        # print(sent_id)\n",
    "        context = mrc_entity_fn[idx]['context']\n",
    "        query = mrc_entity_fn[idx]['query']\n",
    "        tokens = tokenizer.encode(query, context, add_special_tokens=True)\n",
    "\n",
    "        ens_r = ent_r['en']\n",
    "        \n",
    "        for en_r in ens_r:\n",
    "            token_s = en_r[0]\n",
    "            token_e = en_r[1] - 1\n",
    "            # print(en_r[2], token_s,token_e,context)\n",
    "            ent_s, ent_e = remap_index_to_wordindex (token_s, token_e, tokens, context, nnsents, sent_id)\n",
    "            ent_type = en_r[3]\n",
    "            entity_word = txt_contents[ent_s: ent_e]\n",
    "            entities_T.append((ent_type, ent_s, ent_e, entity_word))\n",
    "    \n",
    "    output_t = []\n",
    "    for i, entity_T in enumerate(entities_T):\n",
    "        type, offset_s, offset_e, text = entity_T\n",
    "        if \"\\n\" in text:\n",
    "            text = text.replace(\"\\n\", \" \")\n",
    "        formatted_output_t = output_template_t.format(\"T{}\".format(i), type, offset_s, offset_e, text)\n",
    "        output_t.append (formatted_output_t)\n",
    "\n",
    "    with open(output_fn, \"w\") as f:\n",
    "        formatted_output = \"\\n\".join(output_t)\n",
    "        f.write(formatted_output)\n",
    "        f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "bert_path = \"/home/alexgre/projects/transformer_pretrained_models/bert-large-cased\"\n",
    "vocab_file = os.path.join(bert_path, \"vocab.txt\")\n",
    "tokenizer = BertWordPieceTokenizer(vocab_file)\n",
    "query = 'what are the injuries resulting from a medical intervention related to sulfasalazine'\n",
    "\n",
    "context = \"sulfasalazine dc ' d due to concern for drug induced lupus .\"\n",
    "tokens = tokenizer.encode(query, context, add_special_tokens=True)\n",
    "tokens_1 = tokenizer.encode(context,add_special_tokens=False)\n",
    "start = tokens.token_to_chars(41)\n",
    "print(tokens.tokens)\n",
    "print(tokens.token_to_chars(35))\n",
    "# print(tokens_1.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the output to brat format\n",
    "#RE task\n",
    "BRAT_TEMPLATE_T = \"{}\\t{} {} {}\\t{}\"\n",
    "output_template_t = BRAT_TEMPLATE_T\n",
    "BRAT_TEMPLATE_R = \"{}\\t{}-{} Arg1:{} Arg2:{}\"\n",
    "output_template_r = BRAT_TEMPLATE_R\n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "p_test = Path(\"/data/datasets/cheng/mrc-for-ner-medical/2018_n2c2/test_data/gold_standard_test\")\n",
    "\n",
    "mrc_relation_dir = Path('/data/datasets/cheng/mrc-for-ner-medical/2018_n2c2/data/mrc_relation/mrc-ner.test')\n",
    "\n",
    "relation_pred_dir = Path('/data/datasets/cheng/mrc-for-ner-medical/2018_n2c2/exp/re/pred')\n",
    "\n",
    "relation_model = 'pred_1206_bert-large-cased_2_4_1e-5_20_e2e'\n",
    "\n",
    "relation_pred_fn = relation_pred_dir / f\"{relation_model}.json\"\n",
    "\n",
    "p_output = Path('/data/datasets/cheng/mrc-for-ner-medical/2018_n2c2/exp/re/results/test_e2e')\n",
    "\n",
    "bert_path = \"/home/alexgre/projects/transformer_pretrained_models/bert-large-cased\"\n",
    "vocab_file = os.path.join(bert_path, \"vocab.txt\")\n",
    "tokenizer = BertWordPieceTokenizer(vocab_file)\n",
    "\n",
    "with open(relation_pred_fn, \"r\") as f:\n",
    "    relation_preds = json.load(f)\n",
    "\n",
    "with open(mrc_relation_dir, \"r\") as f:\n",
    "    mrc_relation_fn = json.load(f)\n",
    "\n",
    "file_suffix = 'ann'\n",
    "\n",
    "for i, fn in enumerate(p_test.glob(\"*.txt\")):\n",
    "    txt_fn = p_test / f\"{fn.name}\"\n",
    "    txt_contents = open(txt_fn,'r').read()\n",
    "    \n",
    "    txt, sents = pre_processing(txt_fn, MIMICIII_PATTERN, max_len=256)\n",
    "    output_fn = p_output / \"{}.{}\".format(fn.stem, file_suffix)\n",
    "    \n",
    "    nsents, sent_bound = generate_BIO(sents, [], file_id=\"\", no_overlap=False, record_pos=True)\n",
    "    nnsents = [w for sent in nsents for w in sent]\n",
    "\n",
    "    num_sents = len(nsents)\n",
    "    sent_ids = set(range(num_sents))\n",
    "    print(fn.name)\n",
    "\n",
    "    # ann_fn = p_test / f\"{fn.stem}.ann\"\n",
    "    \n",
    "    # e2i, ens, relations, evns, attrs = load_annotation_brat(ann_fn)\n",
    "    # i2e = {v: k for k, v in e2i.items()}\n",
    "    \n",
    "    # mappings = create_entity_to_sent_mapping(nnsents, ens, i2e, fn.name)\n",
    "\n",
    "\n",
    "    # sent_with_drugs = set()\n",
    "\n",
    "    # drug_lists = []\n",
    "\n",
    "    # sentence with entities\n",
    "    # for en in ens:\n",
    "    #     entype = en[1]\n",
    "    #     type_id = entity_id[entype]\n",
    "    #     text = en[0]\n",
    "    #     s_idx, e_idx = mappings[en[-1]]\n",
    "    #     word_info1 = nnsents[s_idx]\n",
    "    #     word_info2 = nnsents[e_idx]\n",
    "        \n",
    "    #     sent_idx1 = word_info1[3][0]\n",
    "    #     sent_idx2 = word_info2[3][0]\n",
    "    #     sent_with_drugs.add(sent_idx1)\n",
    "        \n",
    "    #     try:\n",
    "    #         sent_text = get_sent(nsents, sent_idx1, sent_idx2)\n",
    "    #         context = \" \".join([e[0] for e in sent_text])\n",
    "    #         start = word_info1[3][1]\n",
    "    #         end = word_info2[3][1]\n",
    "    #         start_end = f\"{start};{end}\"\n",
    "    #     except Exception as ex:\n",
    "    #         print(fn.name)\n",
    "    #         print(word_info1, word_info2)\n",
    "    #         print(context)\n",
    "    #         print(start, end, start_end)\n",
    "    #         print(entype, type_id)\n",
    "            \n",
    "    #         # key will be sent_id, type\n",
    "    #         # data will be tuple (context, entype, type_id, start, end, start_end)\n",
    "    #     if entype == \"Drug\":\n",
    "    #         drug_lists.append(\n",
    "    #             [sent_idx1, text, context, start, end, start_end, en[-1]])\n",
    "\n",
    "    relation_pred = []\n",
    "    event_pred = defaultdict(list)\n",
    "     \n",
    "    for i, pred in enumerate(relation_preds):\n",
    "        sample_idx = pred['sample_idx']\n",
    "        sample_idx = int(sample_idx[0][0])\n",
    "        if sample_idx == int(fn.stem):\n",
    "            relation_pred.append([i,pred])\n",
    "    \n",
    "    # for drug_ent in drug_lists:\n",
    "        \n",
    "    #     head_text = drug_ent[1]\n",
    "    #     head_offset_s = drug_ent[3]\n",
    "    #     head_offset_e = drug_ent[4]\n",
    "    #     head_ann = drug_ent[-1]\n",
    "\n",
    "    for pred_r in relation_pred:\n",
    "        idx = pred_r[0]\n",
    "        re_r = pred_r[1]\n",
    "        head_sent_id = int(re_r['head_sent_idx'][0][0])\n",
    "        tail_sent_id = int(re_r['tail_sent_idx'][0][0])\n",
    "        query = mrc_relation_fn[idx]['query']\n",
    "        head_text = mrc_relation_fn[idx]['entity_label'][1]\n",
    "        head_offset_s = mrc_relation_fn[idx]['entity_label'][2]\n",
    "        head_offset_e = mrc_relation_fn[idx]['entity_label'][3]\n",
    "        \n",
    "        tokens = tokenizer.encode(query, context, add_special_tokens=True)\n",
    "\n",
    "        tails_r = re_r['en']\n",
    "        # head_sent_id = re_r['head_sent_idx'][0][0]\n",
    "        # tail_sent_id = re_r['tail_sent_idx'][0][0]\n",
    "        \n",
    "        for en_r in tails_r: \n",
    "            ent_type = en_r[3]\n",
    "            if head_sent_id == tail_sent_id:\n",
    "                token_s = en_r[0]\n",
    "                token_e = en_r[1] - 1\n",
    "                context = \" \".join(e[0] for e in nsents[head_sent_id])\n",
    "                tokens = tokenizer.encode(query, context, add_special_tokens=True)\n",
    "                # print(context)\n",
    "                # print(query)\n",
    "                # print(tokens.tokens)\n",
    "                # print(head_sent_id, token_s,token_e)\n",
    "                ent_s, ent_e = remap_index_to_wordindex (token_s, token_e, tokens, context, nnsents, head_sent_id)\n",
    "            \n",
    "            if head_sent_id < tail_sent_id:\n",
    "                head_sent = \" \".join(e[0] for e in nsents[head_sent_id])\n",
    "                head_sent_token = tokenizer.encode(head_sent, add_special_tokens=False)\n",
    "                query_token = tokenizer.encode(query, add_special_tokens=False)\n",
    "                query_token_len = len(query_token)\n",
    "                head_sent_len =  len(head_sent_token)\n",
    "                if en_r[0]>= query_token_len + head_sent_len + 2:\n",
    "                    token_s = en_r[0] - head_sent_len\n",
    "                    token_e = en_r[1] -1 -  head_sent_len\n",
    "                    context = \" \".join(e[0] for e in nsents[tail_sent_id])\n",
    "                    tokens = tokenizer.encode(query, context, add_special_tokens=True)\n",
    "                    # print(head_sent_id, tail_sent_id)\n",
    "                    # print(token_s,token_e)\n",
    "                    ent_s, ent_e = remap_index_to_wordindex (token_s, token_e, tokens, context, nnsents, tail_sent_id) \n",
    "\n",
    "            if head_sent_id > tail_sent_id:\n",
    "                tail_sent = \" \".join(e[0] for e in nsents[tail_sent_id])\n",
    "                tail_sent_token = tokenizer.encode(tail_sent, add_special_tokens=False)\n",
    "                query_token = tokenizer.encode(query, add_special_tokens=False)\n",
    "                query_token_len = len(query_token)\n",
    "                tail_sent_len =  len(tail_sent_token)\n",
    "                if en_r[1]-1 < query_token_len + tail_sent_len + 2:\n",
    "                    token_s = en_r[0] \n",
    "                    token_e = en_r[1] -1\n",
    "                    context = \" \".join(e[0] for e in nsents[tail_sent_id])\n",
    "                    tokens = tokenizer.encode(query, context, add_special_tokens=True)\n",
    "                    # print(token_s,token_e)\n",
    "                    ent_s, ent_e = remap_index_to_wordindex (token_s, token_e, tokens, context, nnsents, tail_sent_id) \n",
    "\n",
    "            entity_word = txt_contents[ent_s: ent_e]\n",
    "            \n",
    "            event_pred[(head_text, head_offset_s, head_offset_e)] .append((ent_type, ent_s, ent_e, entity_word))\n",
    "    \n",
    "    output_tr = []\n",
    "    i = 1\n",
    "    k = 1\n",
    "    m = 1\n",
    "    for  event in list(event_pred):\n",
    "        head_ent = event\n",
    "\n",
    "        text_h, head_s, head_e  = head_ent\n",
    "        if \"\\n\" in text_h:\n",
    "            text_h = text_h.replace(\"\\n\", \" \")\n",
    "        formatted_output_head = output_template_t.format(\"T{}\".format(i), 'Drug', head_s, head_e, text_h)\n",
    "        output_tr.append (formatted_output_head)\n",
    "        i= i + 1 \n",
    "\n",
    "        for attributes in event_pred[event]:\n",
    "            att_type, tail_s, tail_e, text_t  = attributes\n",
    "            if \"\\n\" in text_t:\n",
    "                text_t = text_t.replace(\"\\n\", \" \")\n",
    "            formatted_output_att = output_template_t.format(\"T{}\".format(i), att_type, tail_s, tail_e, text_t)\n",
    "            formatted_output_rel = output_template_r.format(\"R{}\".format(m), att_type, 'Drug', \"T{}\".format(i), \"T{}\".format(k))\n",
    "            output_tr.append(formatted_output_att)\n",
    "            output_tr.append(formatted_output_rel)\n",
    "            i = i + 1\n",
    "            m = m + 1\n",
    "        k = i\n",
    "\n",
    "    with open(output_fn, \"w\") as f:\n",
    "        formatted_output = \"\\n\".join(output_tr)\n",
    "        f.write(formatted_output)\n",
    "        f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_fn = '/data/datasets/cheng/mrc-for-ner-medical/2018_n2c2/test_data/test/105585.txt'\n",
    "txt, sents = pre_processing(txt_fn, MIMICIII_PATTERN, max_len=256)\n",
    "e2i, ens, _, evns, attrs = load_annotation_brat(ann_fn)\n",
    "i2e = {v: k for k, v in e2i.items()}\n",
    "nsents, sent_bound = generate_BIO(sents, [], file_id=\"\", no_overlap=False, record_pos=True)\n",
    "nnsents = [w for sent in nsents for w in sent]\n",
    "print(nnsents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_result = Path(\"/data/datasets/cheng/mrc-for-ner-medical/2018_n2c2/exp/re/results/test_1\")\n",
    "p_resultt = Path(\"/data/datasets/cheng/mrc-for-ner-medical/2018_n2c2/exp/re/results/test_single_sentence\")\n",
    "\n",
    "for i, fn in enumerate(p_dev.glob(\"*.ann\")):\n",
    "\n",
    "    txt_fn = p_dev / f\"{fn.stem}.txt\"\n",
    "    ann_fn = p_result / f\"{fn.name}\"\n",
    "    txt, sents = pre_processing(txt_fn, MIMICIII_PATTERN, max_len=256)\n",
    "    e2i, ens, relations, evns, attrs = load_annotation_brat(ann_fn)\n",
    "    i2e = {v: k for k, v in e2i.items()}\n",
    "    nsents, sent_bound = generate_BIO(sents, ens, file_id=\"\", no_overlap=False, record_pos=True)\n",
    "    nnsents = [w for sent in nsents for w in sent]\n",
    "    mappings = create_entity_to_sent_mapping(nnsents, ens, i2e, fn.name)\n",
    "\n",
    "    num_sents = len(nsents)\n",
    "    sent_ids = set(range(num_sents))\n",
    "\n",
    "    ann_file = p_result / f\"{fn.name}\"\n",
    "\n",
    "    file_suffix = 'ann'\n",
    "    output_fn = p_resultt / \"{}.{}\".format(fn.stem, file_suffix)\n",
    "    output = []\n",
    "    with open(ann_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            anns = line.split(\"\\t\")\n",
    "            ann_id = anns[0]\n",
    "            if ann_id.startswith(\"T\"):\n",
    "                output.append(line)\n",
    "            if ann_id.startswith(\"R\"):\n",
    "                relation, tail, head = anns[1].split()[0], anns[1].split()[1], anns[1].split()[-1]\n",
    "                head_ann= mappings[head.split(':')[1]][0]\n",
    "                tail_ann= mappings[tail.split(':')[1]][0]\n",
    "                head_sent_id = nnsents[head_ann][3][0]\n",
    "                tail_sent_id = nnsents[tail_ann][3][0]\n",
    "                if head_sent_id == tail_sent_id:\n",
    "                    output.append(line)\n",
    "    with open(output_fn, \"w\") as f:\n",
    "        formatted_output = \"\\n\".join(output)\n",
    "        f.write(formatted_output)\n",
    "        f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = open('/data/datasets/cheng/mrc-for-ner-medical/2018_n2c2/data/mimic/100035.ann')\n",
    "lines = fi.readlines()\n",
    "print(lines[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f678c47ef45de972fc7e03e8f6b770ee0fe5ec26f99d66888e951db52c49df0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
